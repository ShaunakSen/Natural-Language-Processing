{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation With LSTM Recurrent Neural Networks in Python with Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Deep-Learning/blob/master/Text_Generation_With_LSTM_Recurrent_Neural_Networks_in_Python_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PLy246YtKcOd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text Generation With LSTM Recurrent Neural Networks in Python with Keras\n",
        "\n",
        "[tutorial link](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n",
        "\n",
        "Recurrent neural networks can also be used as generative models.\n",
        "\n",
        "This means that in addition to being used for predictive models (making predictions) they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain.\n",
        "\n",
        "Generative models like this are useful not only to study how well a model has learned a problem, but to learn more about the problem domain itself.\n",
        "\n",
        "In this post you will discover how to create a generative model for text, character-by-character using LSTM recurrent neural networks in Python with Keras.\n",
        "\n",
        "\n",
        "### Problem Description: Project Gutenberg\n",
        "\n",
        "\n",
        "We are going to learn the dependencies between characters and the conditional probabilities of characters in sequences so that we can in turn generate wholly new and original sequences of characters.\n",
        "\n",
        "These experiments are not limited to text, you can also experiment with other ASCII data, such as computer source code, marked up documents in LaTeX, HTML or Markdown and more.\n",
        "\n",
        "### Develop a Small LSTM Recurrent Neural Network\n",
        "\n",
        "In this section we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters.\n",
        "\n",
        "Let’s start off by importing the classes and functions we intend to use to train our model.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7TEbOZS6KaNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ibhcZPP_VcI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UIWTluYtVVcR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "\n",
        "filename = 'alice_data'\n",
        "\n",
        "raw_text = open(file=filename).read()\n",
        "raw_text = raw_text.lower()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1GH3mu7WEGO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
        "\n",
        "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
      ]
    },
    {
      "metadata": {
        "id": "I_KyE1JjWFNN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5da6d7d9-02e0-46dc-9bda-2b9ca9a7ed5f"
      },
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "\n",
        "\n",
        "chars_to_int = dict((c,i) for i,c in enumerate(chars))\n",
        "\n",
        "print (chars_to_int)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, '*': 7, ',': 8, '-': 9, '.': 10, '0': 11, '3': 12, ':': 13, ';': 14, '?': 15, '[': 16, ']': 17, '_': 18, 'a': 19, 'b': 20, 'c': 21, 'd': 22, 'e': 23, 'f': 24, 'g': 25, 'h': 26, 'i': 27, 'j': 28, 'k': 29, 'l': 30, 'm': 31, 'n': 32, 'o': 33, 'p': 34, 'q': 35, 'r': 36, 's': 37, 't': 38, 'u': 39, 'v': 40, 'w': 41, 'x': 42, 'y': 43, 'z': 44}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "osjRlxDfYkpz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "dataset that will reduce the vocabulary and may improve the modeling process.\n",
        "\n",
        "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
      ]
    },
    {
      "metadata": {
        "id": "TYh_TAXUYdt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2430559c-0dd6-423c-fe87-37ce8a3f270a"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144408\n",
            "Total Vocab:  45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "41vy61xKZC_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course).\n",
        "\n",
        "For example, if the sequence length is 5 (for simplicity) then the first two training patterns would be as follows:\n",
        "\n",
        "```\n",
        "CHAPT -> E\n",
        "HAPTE -> R\n",
        "```\n",
        "\n",
        "As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier.\n",
        "\n",
        "\n",
        "Basic idea of the data we want:\n"
      ]
    },
    {
      "metadata": {
        "id": "vPVO9NZ9fjnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b0a4cc79-9865-4610-aa33-75e461f971fb"
      },
      "cell_type": "code",
      "source": [
        "seq_length = 10\n",
        "\n",
        "\n",
        "for i in range(0, 5):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tprint (seq_in, seq_out)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice's ad v\n",
            "lice's adv e\n",
            "ice's adve n\n",
            "ce's adven t\n",
            "e's advent u\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vVutswtJtZrc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So the `X` data should be like `[\"alice's ad\", \"lice's adv\", ...]` and `Y` should be like `[\"v\", \"e\", ...]`. But the chars should not be present, the integere representations of the chars should be present in X and Y\n",
        "\n",
        "\n",
        "Extracting a small part of the datastet for experiment"
      ]
    },
    {
      "metadata": {
        "id": "czkD73s0m5PJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "3a6cb60e-7b34-47cf-9193-725e0488b49f"
      },
      "cell_type": "code",
      "source": [
        "raw_text_sub = raw_text[:100]\n",
        "\n",
        "print(raw_text_sub)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "alice's adventures in wonderland\n",
            "\n",
            "lewis carroll\n",
            "\n",
            "the millennium fulcrum edition 3.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "chapter i. d\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JVnI9s1AmmED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "011c9fca-f4fe-4797-e1ba-39a4bfdb796e"
      },
      "cell_type": "code",
      "source": [
        "text_to_process = raw_text\n",
        "seq_length = 100\n",
        "\n",
        "dataX = []\n",
        "\n",
        "dataY = []\n",
        "\n",
        "n_chars = len(text_to_process)\n",
        "\n",
        "for i in range (0, n_chars-seq_length):\n",
        "  \n",
        "  seq_in = text_to_process[i:i+seq_length]\n",
        "  \n",
        "  seq_out = text_to_process[i+seq_length]\n",
        "  \n",
        "  # print (seq_in, seq_out) \n",
        "  \n",
        "  dataX.append([chars_to_int[char] for char in seq_in])\n",
        "  \n",
        "  dataY.append(chars_to_int[seq_out])\n",
        "  \n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)\n",
        "  "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  144308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8R9ryDt7s6S9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test to see if result matches with tutorial's data \n",
        "# print (dataX == dataX_new and dataY == dataY_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zPQ1tThWtI9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9588c718-7694-4ce5-bd56-a6383095ee9f"
      },
      "cell_type": "code",
      "source": [
        "print (len(dataX), len(dataY))\n",
        "\n",
        "print (len(dataX[0]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144308 144308\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lWtPJCzhvQco",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Running the code to this point shows us that when we split up the dataset into training data for the network to learn that we have just under 150,000 training pattens. This makes sense as excluding the first 100 characters, we have one training pattern to predict each of the remaining characters.\n",
        "\n",
        "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
        "\n",
        "First we must transform the list of input sequences into the form** [samples, time steps, features]** expected by an LSTM network.\n",
        "\n",
        "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
        "\n",
        "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n",
        "\n",
        "For example, when “n” (integer value 31) is one hot encoded it looks as follows:\n",
        "\n",
        "```\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
        "  0.  0.  0.  0.  0.  0.  0.  0.]\n",
        "  ```\n",
        "  \n",
        "We can implement these steps as below."
      ]
    },
    {
      "metadata": {
        "id": "P3SVuohX45pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c2dbee03-e158-4c52-f793-c93996f77841"
      },
      "cell_type": "code",
      "source": [
        "print (len(dataX))\n",
        "\n",
        "print(len(dataX[0]))\n",
        "\n",
        "print (n_vocab)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144308\n",
            "100\n",
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Mlx8oTs5ddt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# [samples, time steps, features]\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "\n",
        "X = X/float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bNyf2k7f6JVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "e47562bf-8bf4-43b2-a550-c0fd08a333a4"
      },
      "cell_type": "code",
      "source": [
        "print (X.shape)\n",
        "\n",
        "print (y.shape)\n",
        "\n",
        "print (y[0])\n",
        "\n",
        "print (X[0][:5])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(144308, 100, 1)\n",
            "(144308, 45)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[[0.42222222]\n",
            " [0.66666667]\n",
            " [0.6       ]\n",
            " [0.46666667]\n",
            " [0.51111111]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKVfTIxN7wE3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20%. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n",
        "\n",
        "\n",
        "The problem is really a single character classification problem with 47 classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kQqxC94c79Ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "7a321aaf-d398-4a6e-bc33-cb7892008c4e"
      },
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units=256, input_shape = (X.shape[1], X.shape[2])))\n",
        "\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HrGjbAXi9LWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
        "\n",
        "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n",
        "\n",
        "The network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LnB4jUyiFaPH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the checkpoint\n",
        "\n",
        "filepath = 'weights-improvement-{epoch:02d}-{loss:.4f}.hdf5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8Gv2KlvG2rg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ne0YKLcxG6f3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1530
        },
        "outputId": "8e0d12ea-465e-4aac-c504-5b158099230b"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x=X, y=y, batch_size=128, epochs=20, callbacks=callbacks_list)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "144308/144308 [==============================] - 163s 1ms/step - loss: 2.9560\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.95602, saving model to weights-improvement-01-2.9560.hdf5\n",
            "Epoch 2/20\n",
            "144308/144308 [==============================] - 153s 1ms/step - loss: 2.7459\n",
            "\n",
            "Epoch 00002: loss improved from 2.95602 to 2.74594, saving model to weights-improvement-02-2.7459.hdf5\n",
            "Epoch 3/20\n",
            "144308/144308 [==============================] - 152s 1ms/step - loss: 2.6428\n",
            "\n",
            "Epoch 00003: loss improved from 2.74594 to 2.64284, saving model to weights-improvement-03-2.6428.hdf5\n",
            "Epoch 4/20\n",
            "144308/144308 [==============================] - 151s 1ms/step - loss: 2.5710\n",
            "\n",
            "Epoch 00004: loss improved from 2.64284 to 2.57104, saving model to weights-improvement-04-2.5710.hdf5\n",
            "Epoch 5/20\n",
            "144308/144308 [==============================] - 152s 1ms/step - loss: 2.5111\n",
            "\n",
            "Epoch 00005: loss improved from 2.57104 to 2.51109, saving model to weights-improvement-05-2.5111.hdf5\n",
            "Epoch 6/20\n",
            "144308/144308 [==============================] - 149s 1ms/step - loss: 2.4524\n",
            "\n",
            "Epoch 00006: loss improved from 2.51109 to 2.45242, saving model to weights-improvement-06-2.4524.hdf5\n",
            "Epoch 7/20\n",
            "144308/144308 [==============================] - 153s 1ms/step - loss: 2.3978\n",
            "\n",
            "Epoch 00007: loss improved from 2.45242 to 2.39778, saving model to weights-improvement-07-2.3978.hdf5\n",
            "Epoch 8/20\n",
            "144308/144308 [==============================] - 151s 1ms/step - loss: 2.3478\n",
            "\n",
            "Epoch 00008: loss improved from 2.39778 to 2.34780, saving model to weights-improvement-08-2.3478.hdf5\n",
            "Epoch 9/20\n",
            "144308/144308 [==============================] - 151s 1ms/step - loss: 2.3006\n",
            "\n",
            "Epoch 00009: loss improved from 2.34780 to 2.30065, saving model to weights-improvement-09-2.3006.hdf5\n",
            "Epoch 10/20\n",
            "144308/144308 [==============================] - 151s 1ms/step - loss: 2.2575\n",
            "\n",
            "Epoch 00010: loss improved from 2.30065 to 2.25754, saving model to weights-improvement-10-2.2575.hdf5\n",
            "Epoch 11/20\n",
            "144308/144308 [==============================] - 150s 1ms/step - loss: 2.2136\n",
            "\n",
            "Epoch 00011: loss improved from 2.25754 to 2.21361, saving model to weights-improvement-11-2.2136.hdf5\n",
            "Epoch 12/20\n",
            "144308/144308 [==============================] - 149s 1ms/step - loss: 2.1764\n",
            "\n",
            "Epoch 00012: loss improved from 2.21361 to 2.17643, saving model to weights-improvement-12-2.1764.hdf5\n",
            "Epoch 13/20\n",
            "144308/144308 [==============================] - 149s 1ms/step - loss: 2.1368\n",
            "\n",
            "Epoch 00013: loss improved from 2.17643 to 2.13676, saving model to weights-improvement-13-2.1368.hdf5\n",
            "Epoch 14/20\n",
            "144308/144308 [==============================] - 148s 1ms/step - loss: 2.1010\n",
            "\n",
            "Epoch 00014: loss improved from 2.13676 to 2.10104, saving model to weights-improvement-14-2.1010.hdf5\n",
            "Epoch 15/20\n",
            "144308/144308 [==============================] - 148s 1ms/step - loss: 2.0647\n",
            "\n",
            "Epoch 00015: loss improved from 2.10104 to 2.06470, saving model to weights-improvement-15-2.0647.hdf5\n",
            "Epoch 16/20\n",
            "144308/144308 [==============================] - 160s 1ms/step - loss: 2.0319\n",
            "\n",
            "Epoch 00016: loss improved from 2.06470 to 2.03191, saving model to weights-improvement-16-2.0319.hdf5\n",
            "Epoch 17/20\n",
            "144308/144308 [==============================] - 152s 1ms/step - loss: 2.0044\n",
            "\n",
            "Epoch 00017: loss improved from 2.03191 to 2.00437, saving model to weights-improvement-17-2.0044.hdf5\n",
            "Epoch 18/20\n",
            "144308/144308 [==============================] - 153s 1ms/step - loss: 1.9710\n",
            "\n",
            "Epoch 00018: loss improved from 2.00437 to 1.97101, saving model to weights-improvement-18-1.9710.hdf5\n",
            "Epoch 19/20\n",
            "144308/144308 [==============================] - 151s 1ms/step - loss: 1.9475\n",
            "\n",
            "Epoch 00019: loss improved from 1.97101 to 1.94747, saving model to weights-improvement-19-1.9475.hdf5\n",
            "Epoch 20/20\n",
            "144308/144308 [==============================] - 153s 1ms/step - loss: 1.9205\n",
            "\n",
            "Epoch 00020: loss improved from 1.94747 to 1.92045, saving model to weights-improvement-20-1.9205.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f312d6dcdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "AhrbtnZUdvwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will see different results because of the stochastic nature of the model, and because it is hard to fix the random seed for LSTM models to get 100% reproducible results. This is not a concern for this generative model.\n",
        "\n",
        "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
        "\n",
        "You can delete them all except the one with the smallest loss value. For example, when I ran this example, below was the checkpoint with the smallest loss that I achieved.\n",
        "\n",
        "`content/weights-improvement-20-1.9205.hdf5`\n",
        "\n",
        "The network loss decreased almost every epoch and I expect the network could benefit from training for many more epochs.\n",
        "\n",
        "In the next section we will look at using this model to generate new text sequences.\n",
        "\n",
        "### Generating Text with an LSTM Network\n",
        "\n",
        "Generating text using the trained LSTM network is relatively straightforward.\n",
        "\n",
        "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZLlrrKmkefRb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load the nw weights\n",
        "\n",
        "filename = 'weights-improvement-20-1.9205.hdf5'\n",
        "\n",
        "model.load_weights(filepath=filename)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2o-6ApdfcYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yZu_onT3fdBj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}