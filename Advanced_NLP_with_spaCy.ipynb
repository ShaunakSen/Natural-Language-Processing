{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced NLP with spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPa3+knnNb2mDuJzEOm1z9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Natural-Language-Processing/blob/master/Advanced_NLP_with_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfRdeWMbqjJ",
        "colab_type": "text"
      },
      "source": [
        "## Advanced NLP with spaCy\n",
        "\n",
        "> Based on the official course: https://course.spacy.io/en\n",
        "\n",
        "---\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n95ciw4RbNxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7wntApobW82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spacy-lookups-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjeW6SGybdKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMfH8WReb1uQ",
        "colab_type": "text"
      },
      "source": [
        "### Chapter 1: Finding words, phrases, names and concepts\n",
        "\n",
        "#### Introduction to spaCy\n",
        "\n",
        "**The nlp object**\n",
        "\n",
        "At the center of spaCy is the object containing the processing pipeline. We usually call this variable \"nlp\".\n",
        "\n",
        "For example, to create an English nlp object, you can import the English language class from `spacy.lang.en` and instantiate it. You can use the nlp object like a function to analyze text.\n",
        "\n",
        "It contains all the different components in the pipeline.\n",
        "\n",
        "It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages that are available in `spacy.lang`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrnSPjWebeuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = English()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV1LCv_ecVtH",
        "colab_type": "text"
      },
      "source": [
        "**The Doc Object**\n",
        "\n",
        "When you process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\n",
        "\n",
        "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index. But more on that later!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjZE51BhcRDk",
        "colab_type": "code",
        "outputId": "c9bdb9ab-92a6-40c6-9528-bfe5cbe54d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Created by processing a string of text with the nlp object\n",
        "\n",
        "doc = nlp(text=\"Hello world!\")\n",
        "\n",
        "# Iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "    print (token.text) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "world\n",
            "!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj-6cPitcxZa",
        "colab_type": "text"
      },
      "source": [
        "**The token object**\n",
        "![](https://course.spacy.io/doc.png)\n",
        "\n",
        "Token objects represent the tokens in a document – for example, a word or a punctuation character.\n",
        "\n",
        "Token objects also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the verbatim token text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy7Pwhr9cu1V",
        "colab_type": "code",
        "outputId": "85e9c676-302a-461e-dd31-dace2b76f810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Index into the Doc to get a single Token\n",
        "token = doc[1]\n",
        "\n",
        "# Get the token text via the .text attribute\n",
        "print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXhgx--kdKuf",
        "colab_type": "text"
      },
      "source": [
        "**The span object**\n",
        "\n",
        "*A Span object is a slice of the document consisting of one or more tokens*. \n",
        "\n",
        "> It's only a view of the Doc and doesn't contain any data itself.\n",
        "\n",
        "To create a span, you can use Python's slice notation. For example, 1:3 will create a slice starting from the token at position 1, up to – but not including! – the token at position 3.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Mvpd27c7lP",
        "colab_type": "code",
        "outputId": "a8024c4a-f0e1-4339-daa6-aeb244dda9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "span = doc[1:3]\n",
        "\n",
        "print(span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPM562uwdpP4",
        "colab_type": "text"
      },
      "source": [
        "**Token attributes**\n",
        "\n",
        "Here you can see some of the available token attributes:\n",
        "\n",
        "i is the index of the token within the parent document.\n",
        "\n",
        "text returns the token text.\n",
        "\n",
        "is_alpha, is_punct and like_num return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
        "\n",
        "These attributes are also called lexical attributes: **they refer to the entry in the vocabulary and don't depend on the token's context**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODlnwwEBdogI",
        "colab_type": "code",
        "outputId": "f22b2427-fd86-4ce4-f662-f0d0b69108ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "doc = nlp(\"It costs $5. Ten £ only\")\n",
        "\n",
        "print(\"Index:   \", [token.i for token in doc])\n",
        "print(\"Text:    \", [token.text for token in doc])\n",
        "\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "print(\"like_num:\", [token.like_num for token in doc])\n",
        "print(\"is_currency:\", [token.is_currency for token in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index:    [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "Text:     ['It', 'costs', '$', '5', '.', 'Ten', '£', 'only']\n",
            "is_alpha: [True, True, False, False, False, True, False, True]\n",
            "is_punct: [False, False, False, False, True, False, False, False]\n",
            "like_num: [False, False, False, True, False, True, False, False]\n",
            "is_currency: [False, False, True, False, False, False, True, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8TX4EoJf4Jm",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises (slightly complicated ones)\n",
        "\n",
        "In this example, you’ll use spaCy’s Doc and Token objects, and lexical attributes to find percentages in a text. You’ll be looking for two subsequent tokens: a number and a percent sign.\n",
        "\n",
        "- Use the like_num token attribute to check whether a token in the doc resembles a number.\n",
        "- Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\n",
        "- Check whether the next token’s text attribute is a percent sign ”%“."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CXda3nHrL8_",
        "colab_type": "code",
        "outputId": "5e624008-e474-444a-9ed2-2566bda73aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Process the text\n",
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "\n",
        "for token in doc:\n",
        "    if token.like_num:\n",
        "        next_token = doc[token.i + 1]\n",
        "        if next_token.text == '%':\n",
        "            print (token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCMTGGqQsfdO",
        "colab_type": "text"
      },
      "source": [
        "#### Statistical models\n",
        "\n",
        "Some of the most interesting things you can analyze are context-specific: for example, whether a word is a verb or whether a span of text is a person name.\n",
        "\n",
        "\n",
        "Statistical models enable spaCy to make predictions in context. This usually includes part-of speech tags, syntactic dependencies and named entities.\n",
        "\n",
        "- Part-of-speech tags\n",
        "- Syntactic dependencies\n",
        "- Named entities\n",
        "\n",
        "- Trained on labeled example texts\n",
        "- Can be updated with more examples to fine-tune predictions\n",
        "\n",
        "spaCy provides a number of pre-trained model packages you can download using the spacy download command. For example, the `\"en_core_web_sm`\" package is a small English model that supports all core capabilities and is trained on web text.\n",
        "\n",
        "The `spacy.load` method loads a model package by name and returns an nlp object.\n",
        "\n",
        "The package provides the binary weights that enable spaCy to make predictions.\n",
        "\n",
        "It also includes the vocabulary, and meta information to tell spaCy which language class to use and how to configure the processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4-yqXzQsgkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP7n1GG0k-sv",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Part-of-speech Tags**\n",
        "\n",
        "1. First, we load the small English model and receive an nlp object.\n",
        "\n",
        "2. Next, we're processing the text \"She ate the pizza\".\n",
        "\n",
        "3. For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag.\n",
        "\n",
        "> In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an integer ID value.\n",
        "\n",
        "Here, the model correctly predicted \"ate\" as a verb and \"pizza\" as a noun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtvGAEu0kx-m",
        "colab_type": "code",
        "outputId": "c89ce92c-6f6b-48b9-ac08-49c56bba3bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "doc = nlp(\"She ate the pizza!\")\n",
        "\n",
        "for token in doc:\n",
        "    print (token.text, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "She PRON\n",
            "ate VERB\n",
            "the DET\n",
            "pizza NOUN\n",
            "! PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwF_ShcylczN",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Syntactic Dependencies**\n",
        "\n",
        "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
        "\n",
        "- The `.dep_` attribute returns the predicted dependency label.\n",
        "\n",
        "- The `.head` attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VClrIwvTlZQY",
        "colab_type": "code",
        "outputId": "d9e9c03e-8eba-4774-9db0-266c0fc54ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "She PRON nsubj ate\n",
            "ate VERB ROOT ate\n",
            "the DET det pizza\n",
            "pizza NOUN dobj ate\n",
            "! PUNCT punct ate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb_Zj7RSmEtl",
        "colab_type": "text"
      },
      "source": [
        "**Dependency label scheme**\n",
        "\n",
        "To describe syntactic dependencies, spaCy uses a standardized label scheme. Here's an example of some common labels:\n",
        "\n",
        "\n",
        "![](https://course.spacy.io/dep_example.png)\n",
        "\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/2nxYpjX/diag1.png)\n",
        "\n",
        "The pronoun \"She\" is a nominal subject attached to the verb – in this case, to \"ate\".\n",
        "\n",
        "The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject, \"she\".\n",
        "\n",
        "The determiner \"the\", also known as an article, is attached to the noun \"pizza\".\n",
        "\n",
        "**Predicting Named Entities**\n",
        "\n",
        "![](https://course.spacy.io/ner_example.png)\n",
        "\n",
        "Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
        "\n",
        "The `doc.ents` property lets you access the named entities predicted by the model.\n",
        "\n",
        "It returns an iterator of Span objects, so we can print the entity text and the entity label using the `.label_` attribute.\n",
        "\n",
        "In this case, the model is correctly predicting \"Apple\" as an organization, \"U.K.\" as a geopolitical entity and \"$1 billion\" as money.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN4bYCMDl3X3",
        "colab_type": "code",
        "outputId": "e3234e98-1a37-42d6-c908-42a2954a4e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW1alWfhroys",
        "colab_type": "text"
      },
      "source": [
        "**Tip: the spacy.explain method**\n",
        "\n",
        "A quick tip: To get definitions for the most common tags and labels, you can use the spacy.explain helper function.\n",
        "\n",
        "For example, \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy.explain can tell you that it refers to countries, cities and states.\n",
        "\n",
        "The same works for part-of-speech tags and dependency labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WNiG2-ErjPI",
        "colab_type": "code",
        "outputId": "4ff467bc-5064-4e00-ef2b-46a6694d0e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print (spacy.explain(\"GPE\"))\n",
        "print (spacy.explain(\"NNP\"))\n",
        "print (spacy.explain(\"dobj\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Countries, cities, states\n",
            "noun, proper singular\n",
            "direct object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9wX7eU337V",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "**Predicting linguistic annotations**\n",
        "\n",
        "You’ll now get to try one of spaCy’s pre-trained model packages and see its predictions in action. Feel free to try it out on your own text! To find out what a tag or label means, you can call spacy.explain in the loop. For example: spacy.explain(\"PROPN\") or spacy.explain(\"GPE\").\n",
        "\n",
        "- Process the text with the nlp object and create a doc.\n",
        "- For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QiNpQXdr-Y4",
        "colab_type": "code",
        "outputId": "6c2bf221-0a6b-448d-f10b-cb4892d66663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    explain_text = str(spacy.explain(term=token_dep))\n",
        "    head_text = token.head.text\n",
        "    # This is for formatting only\n",
        "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}{explain_text:<30}{head_text:<12}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It          PRON      nsubj     nominal subject               official    \n",
            "’s          VERB      punct     punctuation                   It          \n",
            "official    NOUN      ccomp     clausal complement            is          \n",
            ":           PUNCT     punct     punctuation                   is          \n",
            "Apple       PROPN     nsubj     nominal subject               is          \n",
            "is          AUX       ROOT      None                          is          \n",
            "the         DET       det       determiner                    company     \n",
            "first       ADJ       amod      adjectival modifier           company     \n",
            "U.S.        PROPN     nmod      modifier of nominal           company     \n",
            "public      ADJ       amod      adjectival modifier           company     \n",
            "company     NOUN      attr      attribute                     is          \n",
            "to          PART      aux       auxiliary                     reach       \n",
            "reach       VERB      relcl     relative clause modifier      company     \n",
            "a           DET       det       determiner                    value       \n",
            "$           SYM       quantmod  modifier of quantifier        trillion    \n",
            "1           NUM       compound  compound                      trillion    \n",
            "trillion    NUM       nummod    numeric modifier              value       \n",
            "market      NOUN      compound  compound                      value       \n",
            "value       NOUN      dobj      direct object                 reach       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1w3g6rS48hk",
        "colab_type": "text"
      },
      "source": [
        "Note that it also recognized the \"'s\" as an abbr for is, which is a VERB\n",
        "\n",
        "Also note the *$* as symbol and *1* and *trillion* as NUM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOGOYCrs4S-v",
        "colab_type": "code",
        "outputId": "548e62d5-0c19-414a-dd32-750a8ae920c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "first ORDINAL\n",
            "U.S. GPE\n",
            "$1 trillion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCuFuJlT6xUx",
        "colab_type": "text"
      },
      "source": [
        "So far, the model has been correct every single time. In\n",
        "the next exercise, you'll see what happens if the model is wrong, and how to\n",
        "adjust it.\n",
        "\n",
        "**Predicting named entities in context**\n",
        "\n",
        "Models are statistical and not always right. Whether their predictions are correct depends on the training data and the text you’re processing. Let’s take a look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXJuBepj6bgM",
        "colab_type": "code",
        "outputId": "f29ef9ad-32cd-4ffe-8ee9-3d23ade5e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders for $ 1 billion worth Indian customers\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "$ 1 billion MONEY\n",
            "Indian NORP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2bEvLtz7g0p",
        "colab_type": "text"
      },
      "source": [
        "\"iPhone X\" also should be an entity that has been missed by the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejoFGO0S7JgR",
        "colab_type": "code",
        "outputId": "8a9697e6-ebb8-4753-da9d-ff6b3bee8a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "iphone_x = doc[1:3]\n",
        "print (iphone_x.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EL5vpDR8H_J",
        "colab_type": "text"
      },
      "source": [
        "Of course, you don't always have to do this manually. In the\n",
        "next exercise, you'll learn about spaCy's rule-based matcher, which can help you\n",
        "find certain words and phrases in text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWbGZ8PQvD8W",
        "colab_type": "text"
      },
      "source": [
        "#### Rule-based matching\n",
        "\n",
        "Compared to regular expressions, the matcher works with `Doc` and `Token` objects instead of only strings.\n",
        "\n",
        "It's also more flexible: you can search for texts but also other lexical attributes.\n",
        "\n",
        "You can even write rules that use the model's predictions.\n",
        "\n",
        "> For example, find the word \"duck\" only if it's a verb, not a noun.\n",
        "\n",
        "**Match pattern examples**\n",
        "\n",
        "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
        "\n",
        "Match exact token texts:\n",
        "\n",
        "`[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]`\n",
        "\n",
        "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
        "\n",
        "Match lexical attributes:\n",
        "\n",
        "`[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]`\n",
        "\n",
        "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
        "\n",
        "Match any token attributes:\n",
        "\n",
        "`[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]`\n",
        "\n",
        "We can even write patterns using *attributes predicted by the mode*l. Here, we're matching a token with the lemma \"buy\", plus a noun. **The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\"**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQsjwpB973sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load a model and create the nlp object\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw9D58v-wmdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(vocab=nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
        "\n",
        "# Process some text\n",
        "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHkgs9tBz19F",
        "colab_type": "text"
      },
      "source": [
        "To use a pattern, we first import the matcher from spacy.matcher.\n",
        "\n",
        "We also load a model and create the nlp object.\n",
        "\n",
        "The matcher is initialized with the shared vocabulary, nlp.vocab. You'll learn more about this later – for now, just remember to always pass it in.\n",
        "\n",
        "The matcher.add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is an optional callback. We don't need one here, so we set it to None. The third argument is the pattern.\n",
        "\n",
        "To match the pattern on a text, we can call the matcher on any doc.\n",
        "\n",
        "This will return the matches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzhPauIdxdk7",
        "colab_type": "code",
        "outputId": "18f3ec4c-5a72-471c-9442-ea35cd1a686e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print (matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(9528407286733565721, 1, 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw3-6RsL0xn8",
        "colab_type": "text"
      },
      "source": [
        "When you call the matcher on a doc, it returns a list of tuples.\n",
        "\n",
        "Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
        "\n",
        "match_id: hash value of the pattern name\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE6lAgq90sSf",
        "colab_type": "code",
        "outputId": "03f87fa4-3881-4024-91c6-9a812074e9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJRaBBOh6pF2",
        "colab_type": "text"
      },
      "source": [
        "**Matching lexical attributes**\n",
        "\n",
        "Here's an example of a more complex pattern using lexical attributes.\n",
        "\n",
        "We're looking for five tokens:\n",
        "\n",
        "A token consisting of only digits.\n",
        "\n",
        "Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\".\n",
        "\n",
        "And a token that consists of punctuation.\n",
        "\n",
        "The pattern matches the tokens \"2018 FIFA World Cup:\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR5K63903kk",
        "colab_type": "code",
        "outputId": "97dd6b11-7f59-4cd6-b30d-41a7430d1875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pattern_2 = [\n",
        "    {\"IS_DIGIT\": True},\n",
        "    {\"LOWER\": \"fifa\"},\n",
        "    {\"LOWER\": \"world\"},\n",
        "    {\"LOWER\": \"cup\"},\n",
        "    {\"IS_PUNCT\": True}\n",
        "]\n",
        "\n",
        "doc = nlp(\"2018 FIFA World Cup: France won! Upcoming iPhone X release date leaked\")\n",
        "matcher.add(\"FIFA_PATTERN\", None, pattern_2)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(17311505950452258848, 0, 5), (9528407286733565721, 9, 11)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg0Ad6vN7XNB",
        "colab_type": "code",
        "outputId": "2f084ba6-9411-4442-86f6-dacb06406d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018 FIFA World Cup:\n",
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA2-eFuNApR8",
        "colab_type": "text"
      },
      "source": [
        "**Matching other token attributes**\n",
        "\n",
        "In this example, we're looking for two tokens:\n",
        "\n",
        "A verb with the lemma \"love\", followed by a noun.\n",
        "\n",
        "Note: lemma means base word, so the tense will not matter\n",
        "\n",
        "This pattern will match \"loved dogs\" and \"love cats\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MMir2Eb7aQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c31d72c5-a4ee-40ea-a9cd-8eb05d741f3b"
      },
      "source": [
        "pattern = [\n",
        "           {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
        "           {\"POS\": \"NOUN\"}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "\n",
        "\n",
        "matcher.add(\"VERB_PATTERN\", None, pattern)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(14990696005118948706, 1, 3), (14990696005118948706, 6, 8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WDXRscbDRGu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ca3ad356-be99-43d2-bfe0-46024fcdecb5"
      },
      "source": [
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loved dogs\n",
            "love cats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BamduVp2Debr",
        "colab_type": "text"
      },
      "source": [
        "**Using operators and quantifiers**\n",
        "\n",
        "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
        "\n",
        "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpZvlYfLDsUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2a8acee8-804d-4458-dc62-df74268c940a"
      },
      "source": [
        "pattern = [\n",
        "    {\"LEMMA\": \"buy\"},\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "\n",
        "matcher.add(\"OP_PATTERN\", None, pattern)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(15381964648932541590, 1, 4), (15381964648932541590, 8, 10)]\n",
            "bought a smartphone\n",
            "buying apps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhrEFc2XEQ3D",
        "colab_type": "text"
      },
      "source": [
        "\"OP\" can have one of four values:\n",
        "\n",
        "![](https://i.ibb.co/M5LGNMf/diag2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc9eDtb1Ntdv",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAGGMIBeDv9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9a615bc3-0c63-4ec4-e848-4c86bc1f70f2"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"After making the iOS update you won't notice a radical system-wide \"\n",
        "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
        "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
        "    \"some tweaks once you delve a little deeper.\"\n",
        ")\n",
        "\n",
        "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
        "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"IOS_VERSION_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 3\n",
            "Match found: iOS 7\n",
            "Match found: iOS 11\n",
            "Match found: iOS 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueJIjIabWCYZ",
        "colab_type": "text"
      },
      "source": [
        "Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2leXgRUV-5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "07e51c70-bda6-414b-b3f5-1d12634b3db6"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
        "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
        "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
        "    \"I also need to download Winzip? The download speed is very slow\"\n",
        ")\n",
        "\n",
        "# Write a pattern that matches a form of \"download\" plus proper noun\n",
        "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 2\n",
            "Match found: downloaded Fortnite\n",
            "Match found: downloading Minecraft\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXZiiMHWdnI",
        "colab_type": "text"
      },
      "source": [
        "Write one pattern that matches adjectives (\"ADJ\") followed by one or two \"NOUN\"s (one noun and one optional noun).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMb6l_dGWYZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "20fe9c9b-7a4c-44de-a364-4c81346bcb3c"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
        "    \"labels and optional voice responses.\"\n",
        ")\n",
        "\n",
        "# Write a pattern for adjective plus one or two nouns\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 5\n",
            "Match found: beautiful design\n",
            "Match found: smart search\n",
            "Match found: automatic labels\n",
            "Match found: optional voice\n",
            "Match found: optional voice responses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPWmpgkVZiHt",
        "colab_type": "text"
      },
      "source": [
        "#### Data Structures (1): Vocab, Lexemes and StringStore\n",
        "\n",
        "Now that you've had some real experience using spaCy's objects, it's time for you to learn more about what's actually going on under spaCy's hood.\n",
        "\n",
        "In this lesson, we'll take a look at the shared vocabulary and how spaCy deals with strings.\n",
        "\n",
        "**Shared vocab and string store**\n",
        "\n",
        "spaCy stores all shared data in a vocabulary, the Vocab.\n",
        "\n",
        "This includes words, but also the labels schemes for tags and entities.\n",
        "\n",
        "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
        "\n",
        "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp.vocab.strings.\n",
        "\n",
        "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
        "\n",
        "Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idDedrMeXeTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da078492-9dc3-4a0a-c288-0ba757a64a16"
      },
      "source": [
        "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
        "print (coffee_hash)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3197928453018144401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYgP14Pla_jE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "df66f294-2063-465b-d807-53f205cc973a"
      },
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
        "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n",
            "string value: coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDolBoY1bJnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2bf6e21-fc9a-44f7-eb0d-78e4cd6761de"
      },
      "source": [
        "# The doc also exposes the vocab and strings\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rysmAFvhbe85",
        "colab_type": "text"
      },
      "source": [
        "**Lexemes: entries in the vocabulary**\n",
        "\n",
        "Lexemes are **context-independent entries** in the vocabulary.\n",
        "\n",
        "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
        "\n",
        "Lexemes expose attributes, just like tokens.\n",
        "\n",
        "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.\n",
        "\n",
        "*Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4a7BaPQbaoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4a0fb935-8317-4ab4-b978-d2a06bbae7ca"
      },
      "source": [
        "doc = nlp(\"I love mini\")\n",
        "lexemme = nlp.vocab[\"mini\"]\n",
        "\n",
        "print (lexemme)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<spacy.lexeme.Lexeme object at 0x7fe1ca592828>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcwgdyckb56U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "541e1203-716b-45c0-db62-2171198b1ef8"
      },
      "source": [
        "lexemme.is_digit, lexemme.is_alpha, lexemme.text, lexemme.orth"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True, 'mini', 11698860559887369376)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECMpMw-dctQJ",
        "colab_type": "text"
      },
      "source": [
        "Here's an example.\n",
        "\n",
        "The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
        "\n",
        "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store.\n",
        "\n",
        "![](https://i.ibb.co/1m9fBYh/diag3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dfGGmRs9Dt",
        "colab_type": "text"
      },
      "source": [
        "Now that you know all about the vocabulary and string store, we can take a look at the most important data structure: the Doc, and its views Token and Span.\n",
        "\n",
        "**The Doc object**\n",
        "\n",
        "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually.\n",
        "\n",
        "After creating the nlp object, we can import the Doc class from spacy.tokens.\n",
        "\n",
        "Here we're creating a doc from three words. *The spaces are a list of boolean values indicating whether the word is followed by a space*. Every token includes that information – even the last one!\n",
        "\n",
        "The Doc class takes three arguments: the shared vocab, the words and the spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDMzj-osfZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = English()\n",
        "\n",
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfA7XIQtuwX8",
        "colab_type": "text"
      },
      "source": [
        "**The Span obejct**\n",
        "\n",
        "![](https://course.spacy.io/span_indices.png)\n",
        "\n",
        "A Span is a slice of a doc consisting of one or more tokens. The Span takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!\n",
        "\n",
        "To create a Span manually, we can also import the class from spacy.tokens. We can then instantiate it with the doc and the span's start and end index, and an optional label argument.\n",
        "\n",
        "The doc.ents are writable, so we can add entities manually by overwriting it with a list of spans.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9EKFzmauifX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "# Create a span manually\n",
        "span = Span(doc, 0, 2)\n",
        "\n",
        "# Create a span with a label\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQN1qbSy7c7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "db980d3d-4243-4b1c-af60-cceef16ee659"
      },
      "source": [
        "for token in span_with_label:\n",
        "    print (token.text)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zex3dhP_8ZES",
        "colab_type": "text"
      },
      "source": [
        "The text \"Hello world!\" is like a GREETING (entity). So we add it to the doc entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es1zuTdT7hdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4921f0f0-7e98-4b95-d960-f9c5cfe969c9"
      },
      "source": [
        "# Add span to the doc.ents\n",
        "doc.ents = [span_with_label]\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world GREETING\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n2Tmov_8YBC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAG0FuOZ8kLc",
        "colab_type": "text"
      },
      "source": [
        "**Best practices**\n",
        "\n",
        "The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
        "\n",
        "If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
        "\n",
        "To keep things consistent, try to use built-in token attributes wherever possible. For example, token.i for the token index.\n",
        "\n",
        "Also, don't forget to always pass in the shared vocab!\n",
        "\n",
        "#### Exercises\n",
        "\n",
        "In this exercise, you’ll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created.\n",
        "\n",
        "- Import the Doc and Span classes from spacy.tokens.\n",
        "- Use the Doc class directly to create a doc from the words and spaces.\n",
        "- Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
        "- Overwrite the doc.ents with a list of one entity, the “David Bowie” span."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwRBKEZK7_C5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3491f973-3b63-4acf-f116-117f7dae2f13"
      },
      "source": [
        "nlp = English()\n",
        "\n",
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words, spaces)\n",
        "print(doc.text)\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label=\"PERSON\")\n",
        "print(span.text, span.label_)\n",
        "\n",
        "# Add the span to the doc's entities\n",
        "doc.ents = [span]\n",
        "\n",
        "# Print entities' text and labels\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like David Bowie\n",
            "David Bowie PERSON\n",
            "[('David Bowie', 'PERSON')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOI58dOICPLh",
        "colab_type": "text"
      },
      "source": [
        "**Data Structures Best Practices**\n",
        "\n",
        "The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM87RlBUA0G0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b72cfa8-7632-444a-b51e-0a7eec6a2acc"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin is a nice city\")\n",
        "\n",
        "# Get all tokens and part-of-speech tags\n",
        "token_texts = [token.text for token in doc]\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "print (pos_tags)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYY2m5EZCW72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, pos in enumerate(pos_tags):\n",
        "    # Check if the current token is a proper noun\n",
        "    if pos == 'PROPN':\n",
        "        # Check if the next token is a verb\n",
        "        if pos_tags[index+1] == 'VERB':\n",
        "            result = token_texts[index]\n",
        "            print(\"Found proper noun before a verb:\", result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYmsDZ_lCy-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fda3f252-5428-44cc-d90a-59bdc5f45037"
      },
      "source": [
        "[token.tag_ for token in doc]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NNP', 'VBZ', 'DT', 'JJ', 'NN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jrjaDsPDUXk",
        "colab_type": "text"
      },
      "source": [
        "Why is the code bad?\n",
        "\n",
        "- It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeFJz-tpDE_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "957fbc48-e706-4529-c54b-0e0bde87c0ae"
      },
      "source": [
        "type(token_texts[0])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyns7VU3Dfvp",
        "colab_type": "text"
      },
      "source": [
        "- Rewrite the code to use the native token attributes instead of lists of token_texts and pos_tags.\n",
        "- Loop over each token in the doc and check the token.pos_ attribute.\n",
        "- Use doc[token.i + 1] to check for the next token and its .pos_ attribute.\n",
        "- If a proper noun before a verb is found, print its token.text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqALQHRwD2ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "15397853-b72e-439b-a5e0-50f301a0844e"
      },
      "source": [
        "doc = nlp(\"Berlin depicts nice city Tokyo\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\" and token.i+1 < len(doc):\n",
        "        if doc[token.i + 1].pos_ == \"VERB\":\n",
        "            print (\"Found proper noun before a verb:\", token.text)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found proper noun before a verb: Berlin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3aQ-eAPEcy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}