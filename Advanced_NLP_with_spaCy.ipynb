{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced NLP with spaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM24U4FPAFFaaPgJ3v4FHq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Natural-Language-Processing/blob/master/Advanced_NLP_with_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfRdeWMbqjJ",
        "colab_type": "text"
      },
      "source": [
        "## Advanced NLP with spaCy\n",
        "\n",
        "> Based on the official course: https://course.spacy.io/en\n",
        "\n",
        "---\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n95ciw4RbNxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7wntApobW82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U spacy-lookups-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjeW6SGybdKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iODhyPwx2lgD",
        "colab_type": "code",
        "outputId": "f8026996-aa7e-4366-980a-74a8a3f89317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMfH8WReb1uQ",
        "colab_type": "text"
      },
      "source": [
        "### Chapter 1: Finding words, phrases, names and concepts\n",
        "\n",
        "#### Introduction to spaCy\n",
        "\n",
        "**The nlp object**\n",
        "\n",
        "At the center of spaCy is the object containing the processing pipeline. We usually call this variable \"nlp\".\n",
        "\n",
        "For example, to create an English nlp object, you can import the English language class from `spacy.lang.en` and instantiate it. You can use the nlp object like a function to analyze text.\n",
        "\n",
        "It contains all the different components in the pipeline.\n",
        "\n",
        "It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages that are available in `spacy.lang`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrnSPjWebeuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the English language class\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create the nlp object\n",
        "nlp = English()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV1LCv_ecVtH",
        "colab_type": "text"
      },
      "source": [
        "**The Doc Object**\n",
        "\n",
        "When you process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\n",
        "\n",
        "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index. But more on that later!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjZE51BhcRDk",
        "colab_type": "code",
        "outputId": "c9bdb9ab-92a6-40c6-9528-bfe5cbe54d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Created by processing a string of text with the nlp object\n",
        "\n",
        "doc = nlp(text=\"Hello world!\")\n",
        "\n",
        "# Iterate over tokens in a Doc\n",
        "for token in doc:\n",
        "    print (token.text) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "world\n",
            "!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj-6cPitcxZa",
        "colab_type": "text"
      },
      "source": [
        "**The token object**\n",
        "![](https://course.spacy.io/doc.png)\n",
        "\n",
        "Token objects represent the tokens in a document – for example, a word or a punctuation character.\n",
        "\n",
        "Token objects also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the verbatim token text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy7Pwhr9cu1V",
        "colab_type": "code",
        "outputId": "85e9c676-302a-461e-dd31-dace2b76f810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Index into the Doc to get a single Token\n",
        "token = doc[1]\n",
        "\n",
        "# Get the token text via the .text attribute\n",
        "print(token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXhgx--kdKuf",
        "colab_type": "text"
      },
      "source": [
        "**The span object**\n",
        "\n",
        "*A Span object is a slice of the document consisting of one or more tokens*. \n",
        "\n",
        "> It's only a view of the Doc and doesn't contain any data itself.\n",
        "\n",
        "To create a span, you can use Python's slice notation. For example, 1:3 will create a slice starting from the token at position 1, up to – but not including! – the token at position 3.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4Mvpd27c7lP",
        "colab_type": "code",
        "outputId": "a8024c4a-f0e1-4339-daa6-aeb244dda9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "span = doc[1:3]\n",
        "\n",
        "print(span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "world!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPM562uwdpP4",
        "colab_type": "text"
      },
      "source": [
        "**Token attributes**\n",
        "\n",
        "Here you can see some of the available token attributes:\n",
        "\n",
        "i is the index of the token within the parent document.\n",
        "\n",
        "text returns the token text.\n",
        "\n",
        "is_alpha, is_punct and like_num return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\n",
        "\n",
        "These attributes are also called lexical attributes: **they refer to the entry in the vocabulary and don't depend on the token's context**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODlnwwEBdogI",
        "colab_type": "code",
        "outputId": "f22b2427-fd86-4ce4-f662-f0d0b69108ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "doc = nlp(\"It costs $5. Ten £ only\")\n",
        "\n",
        "print(\"Index:   \", [token.i for token in doc])\n",
        "print(\"Text:    \", [token.text for token in doc])\n",
        "\n",
        "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
        "print(\"is_punct:\", [token.is_punct for token in doc])\n",
        "print(\"like_num:\", [token.like_num for token in doc])\n",
        "print(\"is_currency:\", [token.is_currency for token in doc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index:    [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "Text:     ['It', 'costs', '$', '5', '.', 'Ten', '£', 'only']\n",
            "is_alpha: [True, True, False, False, False, True, False, True]\n",
            "is_punct: [False, False, False, False, True, False, False, False]\n",
            "like_num: [False, False, False, True, False, True, False, False]\n",
            "is_currency: [False, False, True, False, False, False, True, False]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8TX4EoJf4Jm",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises (slightly complicated ones)\n",
        "\n",
        "In this example, you’ll use spaCy’s Doc and Token objects, and lexical attributes to find percentages in a text. You’ll be looking for two subsequent tokens: a number and a percent sign.\n",
        "\n",
        "- Use the like_num token attribute to check whether a token in the doc resembles a number.\n",
        "- Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\n",
        "- Check whether the next token’s text attribute is a percent sign ”%“."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CXda3nHrL8_",
        "colab_type": "code",
        "outputId": "5e624008-e474-444a-9ed2-2566bda73aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Process the text\n",
        "doc = nlp(\n",
        "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
        "    \"Now less than 4% are.\"\n",
        ")\n",
        "\n",
        "for token in doc:\n",
        "    if token.like_num:\n",
        "        next_token = doc[token.i + 1]\n",
        "        if next_token.text == '%':\n",
        "            print (token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCMTGGqQsfdO",
        "colab_type": "text"
      },
      "source": [
        "#### Statistical models\n",
        "\n",
        "Some of the most interesting things you can analyze are context-specific: for example, whether a word is a verb or whether a span of text is a person name.\n",
        "\n",
        "\n",
        "Statistical models enable spaCy to make predictions in context. This usually includes part-of speech tags, syntactic dependencies and named entities.\n",
        "\n",
        "- Part-of-speech tags\n",
        "- Syntactic dependencies\n",
        "- Named entities\n",
        "\n",
        "- Trained on labeled example texts\n",
        "- Can be updated with more examples to fine-tune predictions\n",
        "\n",
        "spaCy provides a number of pre-trained model packages you can download using the spacy download command. For example, the `\"en_core_web_sm`\" package is a small English model that supports all core capabilities and is trained on web text.\n",
        "\n",
        "The `spacy.load` method loads a model package by name and returns an nlp object.\n",
        "\n",
        "The package provides the binary weights that enable spaCy to make predictions.\n",
        "\n",
        "It also includes the vocabulary, and meta information to tell spaCy which language class to use and how to configure the processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4-yqXzQsgkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP7n1GG0k-sv",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Part-of-speech Tags**\n",
        "\n",
        "1. First, we load the small English model and receive an nlp object.\n",
        "\n",
        "2. Next, we're processing the text \"She ate the pizza\".\n",
        "\n",
        "3. For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag.\n",
        "\n",
        "> In spaCy, attributes that return strings usually end with an underscore – attributes without the underscore return an integer ID value.\n",
        "\n",
        "Here, the model correctly predicted \"ate\" as a verb and \"pizza\" as a noun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtvGAEu0kx-m",
        "colab_type": "code",
        "outputId": "c89ce92c-6f6b-48b9-ac08-49c56bba3bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "doc = nlp(\"She ate the pizza!\")\n",
        "\n",
        "for token in doc:\n",
        "    print (token.text, token.pos_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "She PRON\n",
            "ate VERB\n",
            "the DET\n",
            "pizza NOUN\n",
            "! PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwF_ShcylczN",
        "colab_type": "text"
      },
      "source": [
        "**Predicting Syntactic Dependencies**\n",
        "\n",
        "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object.\n",
        "\n",
        "- The `.dep_` attribute returns the predicted dependency label.\n",
        "\n",
        "- The `.head` attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VClrIwvTlZQY",
        "colab_type": "code",
        "outputId": "d9e9c03e-8eba-4774-9db0-266c0fc54ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "She PRON nsubj ate\n",
            "ate VERB ROOT ate\n",
            "the DET det pizza\n",
            "pizza NOUN dobj ate\n",
            "! PUNCT punct ate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb_Zj7RSmEtl",
        "colab_type": "text"
      },
      "source": [
        "**Dependency label scheme**\n",
        "\n",
        "To describe syntactic dependencies, spaCy uses a standardized label scheme. Here's an example of some common labels:\n",
        "\n",
        "\n",
        "![](https://course.spacy.io/dep_example.png)\n",
        "\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/2nxYpjX/diag1.png)\n",
        "\n",
        "The pronoun \"She\" is a nominal subject attached to the verb – in this case, to \"ate\".\n",
        "\n",
        "The noun \"pizza\" is a direct object attached to the verb \"ate\". It is eaten by the subject, \"she\".\n",
        "\n",
        "The determiner \"the\", also known as an article, is attached to the noun \"pizza\".\n",
        "\n",
        "**Predicting Named Entities**\n",
        "\n",
        "![](https://course.spacy.io/ner_example.png)\n",
        "\n",
        "Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
        "\n",
        "The `doc.ents` property lets you access the named entities predicted by the model.\n",
        "\n",
        "It returns an iterator of Span objects, so we can print the entity text and the entity label using the `.label_` attribute.\n",
        "\n",
        "In this case, the model is correctly predicting \"Apple\" as an organization, \"U.K.\" as a geopolitical entity and \"$1 billion\" as money.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN4bYCMDl3X3",
        "colab_type": "code",
        "outputId": "e3234e98-1a37-42d6-c908-42a2954a4e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Process a text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW1alWfhroys",
        "colab_type": "text"
      },
      "source": [
        "**Tip: the spacy.explain method**\n",
        "\n",
        "A quick tip: To get definitions for the most common tags and labels, you can use the spacy.explain helper function.\n",
        "\n",
        "For example, \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy.explain can tell you that it refers to countries, cities and states.\n",
        "\n",
        "The same works for part-of-speech tags and dependency labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WNiG2-ErjPI",
        "colab_type": "code",
        "outputId": "4ff467bc-5064-4e00-ef2b-46a6694d0e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print (spacy.explain(\"GPE\"))\n",
        "print (spacy.explain(\"NNP\"))\n",
        "print (spacy.explain(\"dobj\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Countries, cities, states\n",
            "noun, proper singular\n",
            "direct object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu9wX7eU337V",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "**Predicting linguistic annotations**\n",
        "\n",
        "You’ll now get to try one of spaCy’s pre-trained model packages and see its predictions in action. Feel free to try it out on your own text! To find out what a tag or label means, you can call spacy.explain in the loop. For example: spacy.explain(\"PROPN\") or spacy.explain(\"GPE\").\n",
        "\n",
        "- Process the text with the nlp object and create a doc.\n",
        "- For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QiNpQXdr-Y4",
        "colab_type": "code",
        "outputId": "6c2bf221-0a6b-448d-f10b-cb4892d66663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    explain_text = str(spacy.explain(term=token_dep))\n",
        "    head_text = token.head.text\n",
        "    # This is for formatting only\n",
        "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}{explain_text:<30}{head_text:<12}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It          PRON      nsubj     nominal subject               official    \n",
            "’s          VERB      punct     punctuation                   It          \n",
            "official    NOUN      ccomp     clausal complement            is          \n",
            ":           PUNCT     punct     punctuation                   is          \n",
            "Apple       PROPN     nsubj     nominal subject               is          \n",
            "is          AUX       ROOT      None                          is          \n",
            "the         DET       det       determiner                    company     \n",
            "first       ADJ       amod      adjectival modifier           company     \n",
            "U.S.        PROPN     nmod      modifier of nominal           company     \n",
            "public      ADJ       amod      adjectival modifier           company     \n",
            "company     NOUN      attr      attribute                     is          \n",
            "to          PART      aux       auxiliary                     reach       \n",
            "reach       VERB      relcl     relative clause modifier      company     \n",
            "a           DET       det       determiner                    value       \n",
            "$           SYM       quantmod  modifier of quantifier        trillion    \n",
            "1           NUM       compound  compound                      trillion    \n",
            "trillion    NUM       nummod    numeric modifier              value       \n",
            "market      NOUN      compound  compound                      value       \n",
            "value       NOUN      dobj      direct object                 reach       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1w3g6rS48hk",
        "colab_type": "text"
      },
      "source": [
        "Note that it also recognized the \"'s\" as an abbr for is, which is a VERB\n",
        "\n",
        "Also note the *$* as symbol and *1* and *trillion* as NUM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOGOYCrs4S-v",
        "colab_type": "code",
        "outputId": "548e62d5-0c19-414a-dd32-750a8ae920c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "first ORDINAL\n",
            "U.S. GPE\n",
            "$1 trillion MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCuFuJlT6xUx",
        "colab_type": "text"
      },
      "source": [
        "So far, the model has been correct every single time. In\n",
        "the next exercise, you'll see what happens if the model is wrong, and how to\n",
        "adjust it.\n",
        "\n",
        "**Predicting named entities in context**\n",
        "\n",
        "Models are statistical and not always right. Whether their predictions are correct depends on the training data and the text you’re processing. Let’s take a look at an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXJuBepj6bgM",
        "colab_type": "code",
        "outputId": "f29ef9ad-32cd-4ffe-8ee9-3d23ade5e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders for $ 1 billion worth Indian customers\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple ORG\n",
            "$ 1 billion MONEY\n",
            "Indian NORP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2bEvLtz7g0p",
        "colab_type": "text"
      },
      "source": [
        "\"iPhone X\" also should be an entity that has been missed by the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejoFGO0S7JgR",
        "colab_type": "code",
        "outputId": "8a9697e6-ebb8-4753-da9d-ff6b3bee8a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "iphone_x = doc[1:3]\n",
        "print (iphone_x.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EL5vpDR8H_J",
        "colab_type": "text"
      },
      "source": [
        "Of course, you don't always have to do this manually. In the\n",
        "next exercise, you'll learn about spaCy's rule-based matcher, which can help you\n",
        "find certain words and phrases in text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWbGZ8PQvD8W",
        "colab_type": "text"
      },
      "source": [
        "#### Rule-based matching\n",
        "\n",
        "Compared to regular expressions, the matcher works with `Doc` and `Token` objects instead of only strings.\n",
        "\n",
        "It's also more flexible: you can search for texts but also other lexical attributes.\n",
        "\n",
        "You can even write rules that use the model's predictions.\n",
        "\n",
        "> For example, find the word \"duck\" only if it's a verb, not a noun.\n",
        "\n",
        "**Match pattern examples**\n",
        "\n",
        "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
        "\n",
        "Match exact token texts:\n",
        "\n",
        "`[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]`\n",
        "\n",
        "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
        "\n",
        "Match lexical attributes:\n",
        "\n",
        "`[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]`\n",
        "\n",
        "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
        "\n",
        "Match any token attributes:\n",
        "\n",
        "`[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]`\n",
        "\n",
        "We can even write patterns using *attributes predicted by the mode*l. Here, we're matching a token with the lemma \"buy\", plus a noun. **The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\"**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQsjwpB973sb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load a model and create the nlp object\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw9D58v-wmdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the matcher with the shared vocab\n",
        "matcher = Matcher(vocab=nlp.vocab)\n",
        "\n",
        "# Add the pattern to the matcher\n",
        "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
        "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
        "\n",
        "# Process some text\n",
        "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHkgs9tBz19F",
        "colab_type": "text"
      },
      "source": [
        "To use a pattern, we first import the matcher from spacy.matcher.\n",
        "\n",
        "We also load a model and create the nlp object.\n",
        "\n",
        "The matcher is initialized with the shared vocabulary, nlp.vocab. You'll learn more about this later – for now, just remember to always pass it in.\n",
        "\n",
        "The matcher.add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is an optional callback. We don't need one here, so we set it to None. The third argument is the pattern.\n",
        "\n",
        "To match the pattern on a text, we can call the matcher on any doc.\n",
        "\n",
        "This will return the matches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzhPauIdxdk7",
        "colab_type": "code",
        "outputId": "18f3ec4c-5a72-471c-9442-ea35cd1a686e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print (matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(9528407286733565721, 1, 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw3-6RsL0xn8",
        "colab_type": "text"
      },
      "source": [
        "When you call the matcher on a doc, it returns a list of tuples.\n",
        "\n",
        "Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
        "\n",
        "match_id: hash value of the pattern name\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE6lAgq90sSf",
        "colab_type": "code",
        "outputId": "03f87fa4-3881-4024-91c6-9a812074e9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Iterate over the matches\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJRaBBOh6pF2",
        "colab_type": "text"
      },
      "source": [
        "**Matching lexical attributes**\n",
        "\n",
        "Here's an example of a more complex pattern using lexical attributes.\n",
        "\n",
        "We're looking for five tokens:\n",
        "\n",
        "A token consisting of only digits.\n",
        "\n",
        "Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\".\n",
        "\n",
        "And a token that consists of punctuation.\n",
        "\n",
        "The pattern matches the tokens \"2018 FIFA World Cup:\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvR5K63903kk",
        "colab_type": "code",
        "outputId": "97dd6b11-7f59-4cd6-b30d-41a7430d1875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pattern_2 = [\n",
        "    {\"IS_DIGIT\": True},\n",
        "    {\"LOWER\": \"fifa\"},\n",
        "    {\"LOWER\": \"world\"},\n",
        "    {\"LOWER\": \"cup\"},\n",
        "    {\"IS_PUNCT\": True}\n",
        "]\n",
        "\n",
        "doc = nlp(\"2018 FIFA World Cup: France won! Upcoming iPhone X release date leaked\")\n",
        "matcher.add(\"FIFA_PATTERN\", None, pattern_2)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(17311505950452258848, 0, 5), (9528407286733565721, 9, 11)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg0Ad6vN7XNB",
        "colab_type": "code",
        "outputId": "2f084ba6-9411-4442-86f6-dacb06406d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018 FIFA World Cup:\n",
            "iPhone X\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA2-eFuNApR8",
        "colab_type": "text"
      },
      "source": [
        "**Matching other token attributes**\n",
        "\n",
        "In this example, we're looking for two tokens:\n",
        "\n",
        "A verb with the lemma \"love\", followed by a noun.\n",
        "\n",
        "Note: lemma means base word, so the tense will not matter\n",
        "\n",
        "This pattern will match \"loved dogs\" and \"love cats\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MMir2Eb7aQf",
        "colab_type": "code",
        "outputId": "c31d72c5-a4ee-40ea-a9cd-8eb05d741f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pattern = [\n",
        "           {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
        "           {\"POS\": \"NOUN\"}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
        "\n",
        "\n",
        "matcher.add(\"VERB_PATTERN\", None, pattern)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(14990696005118948706, 1, 3), (14990696005118948706, 6, 8)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WDXRscbDRGu",
        "colab_type": "code",
        "outputId": "ca3ad356-be99-43d2-bfe0-46024fcdecb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loved dogs\n",
            "love cats\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BamduVp2Debr",
        "colab_type": "text"
      },
      "source": [
        "**Using operators and quantifiers**\n",
        "\n",
        "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
        "\n",
        "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpZvlYfLDsUx",
        "colab_type": "code",
        "outputId": "2a8acee8-804d-4458-dc62-df74268c940a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "pattern = [\n",
        "    {\"LEMMA\": \"buy\"},\n",
        "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
        "    {\"POS\": \"NOUN\"}\n",
        "]\n",
        "\n",
        "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
        "\n",
        "matcher.add(\"OP_PATTERN\", None, pattern)\n",
        "\n",
        "# Call the matcher on the doc\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    # Get the matched span\n",
        "    matched_span = doc[start:end]\n",
        "    print(matched_span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(15381964648932541590, 1, 4), (15381964648932541590, 8, 10)]\n",
            "bought a smartphone\n",
            "buying apps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhrEFc2XEQ3D",
        "colab_type": "text"
      },
      "source": [
        "\"OP\" can have one of four values:\n",
        "\n",
        "![](https://i.ibb.co/M5LGNMf/diag2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc9eDtb1Ntdv",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAGGMIBeDv9j",
        "colab_type": "code",
        "outputId": "9a615bc3-0c63-4ec4-e848-4c86bc1f70f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"After making the iOS update you won't notice a radical system-wide \"\n",
        "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
        "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
        "    \"some tweaks once you delve a little deeper.\"\n",
        ")\n",
        "\n",
        "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
        "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"IOS_VERSION_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 3\n",
            "Match found: iOS 7\n",
            "Match found: iOS 11\n",
            "Match found: iOS 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueJIjIabWCYZ",
        "colab_type": "text"
      },
      "source": [
        "Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2leXgRUV-5h",
        "colab_type": "code",
        "outputId": "07e51c70-bda6-414b-b3f5-1d12634b3db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
        "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
        "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
        "    \"I also need to download Winzip? The download speed is very slow\"\n",
        ")\n",
        "\n",
        "# Write a pattern that matches a form of \"download\" plus proper noun\n",
        "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 2\n",
            "Match found: downloaded Fortnite\n",
            "Match found: downloading Minecraft\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXZiiMHWdnI",
        "colab_type": "text"
      },
      "source": [
        "Write one pattern that matches adjectives (\"ADJ\") followed by one or two \"NOUN\"s (one noun and one optional noun).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMb6l_dGWYZ0",
        "colab_type": "code",
        "outputId": "20fe9c9b-7a4c-44de-a364-4c81346bcb3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "doc = nlp(\n",
        "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
        "    \"labels and optional voice responses.\"\n",
        ")\n",
        "\n",
        "# Write a pattern for adjective plus one or two nouns\n",
        "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]\n",
        "\n",
        "# Add the pattern to the matcher and apply the matcher to the doc\n",
        "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
        "matches = matcher(doc)\n",
        "print(\"Total matches found:\", len(matches))\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total matches found: 5\n",
            "Match found: beautiful design\n",
            "Match found: smart search\n",
            "Match found: automatic labels\n",
            "Match found: optional voice\n",
            "Match found: optional voice responses\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPWmpgkVZiHt",
        "colab_type": "text"
      },
      "source": [
        "### Chapter 2: Large-scale data analysis with spaCy\n",
        "\n",
        "#### Data Structures (1): Vocab, Lexemes and StringStore\n",
        "\n",
        "Now that you've had some real experience using spaCy's objects, it's time for you to learn more about what's actually going on under spaCy's hood.\n",
        "\n",
        "In this lesson, we'll take a look at the shared vocabulary and how spaCy deals with strings.\n",
        "\n",
        "**Shared vocab and string store**\n",
        "\n",
        "spaCy stores all shared data in a vocabulary, the Vocab.\n",
        "\n",
        "This includes words, but also the labels schemes for tags and entities.\n",
        "\n",
        "To save memory, all strings are encoded to hash IDs. If a word occurs more than once, we don't need to save it every time.\n",
        "\n",
        "Instead, spaCy uses a hash function to generate an ID and stores the string only once in the string store. The string store is available as nlp.vocab.strings.\n",
        "\n",
        "It's a lookup table that works in both directions. You can look up a string and get its hash, and look up a hash to get its string value. Internally, spaCy only communicates in hash IDs.\n",
        "\n",
        "Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idDedrMeXeTA",
        "colab_type": "code",
        "outputId": "da078492-9dc3-4a0a-c288-0ba757a64a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
        "print (coffee_hash)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3197928453018144401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYgP14Pla_jE",
        "colab_type": "code",
        "outputId": "df66f294-2063-465b-d807-53f205cc973a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
        "print(\"string value:\", nlp.vocab.strings[3197928453018144401])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n",
            "string value: coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDolBoY1bJnW",
        "colab_type": "code",
        "outputId": "c2bf6e21-fc9a-44f7-eb0d-78e4cd6761de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The doc also exposes the vocab and strings\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hash value: 3197928453018144401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rysmAFvhbe85",
        "colab_type": "text"
      },
      "source": [
        "**Lexemes: entries in the vocabulary**\n",
        "\n",
        "Lexemes are **context-independent entries** in the vocabulary.\n",
        "\n",
        "You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
        "\n",
        "Lexemes expose attributes, just like tokens.\n",
        "\n",
        "They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.\n",
        "\n",
        "*Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4a7BaPQbaoK",
        "colab_type": "code",
        "outputId": "4a0fb935-8317-4ab4-b978-d2a06bbae7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc = nlp(\"I love mini\")\n",
        "lexemme = nlp.vocab[\"mini\"]\n",
        "\n",
        "print (lexemme)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<spacy.lexeme.Lexeme object at 0x7fe1ca592828>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcwgdyckb56U",
        "colab_type": "code",
        "outputId": "541e1203-716b-45c0-db62-2171198b1ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lexemme.is_digit, lexemme.is_alpha, lexemme.text, lexemme.orth"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True, 'mini', 11698860559887369376)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECMpMw-dctQJ",
        "colab_type": "text"
      },
      "source": [
        "Here's an example.\n",
        "\n",
        "The Doc contains words in context – in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies.\n",
        "\n",
        "Each token refers to a lexeme, which knows the word's hash ID. To get the string representation of the word, spaCy looks up the hash in the string store.\n",
        "\n",
        "![](https://i.ibb.co/1m9fBYh/diag3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dfGGmRs9Dt",
        "colab_type": "text"
      },
      "source": [
        "Now that you know all about the vocabulary and string store, we can take a look at the most important data structure: the Doc, and its views Token and Span.\n",
        "\n",
        "**The Doc object**\n",
        "\n",
        "The Doc is one of the central data structures in spaCy. It's created automatically when you process a text with the nlp object. But you can also instantiate the class manually.\n",
        "\n",
        "After creating the nlp object, we can import the Doc class from spacy.tokens.\n",
        "\n",
        "Here we're creating a doc from three words. *The spaces are a list of boolean values indicating whether the word is followed by a space*. Every token includes that information – even the last one!\n",
        "\n",
        "The Doc class takes three arguments: the shared vocab, the words and the spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNDMzj-osfZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = English()\n",
        "\n",
        "# Import the Doc class\n",
        "from spacy.tokens import Doc\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfA7XIQtuwX8",
        "colab_type": "text"
      },
      "source": [
        "**The Span obejct**\n",
        "\n",
        "![](https://course.spacy.io/span_indices.png)\n",
        "\n",
        "A Span is a slice of a doc consisting of one or more tokens. The Span takes at least three arguments: the doc it refers to, and the start and end index of the span. Remember that the end index is exclusive!\n",
        "\n",
        "To create a Span manually, we can also import the class from spacy.tokens. We can then instantiate it with the doc and the span's start and end index, and an optional label argument.\n",
        "\n",
        "The doc.ents are writable, so we can add entities manually by overwriting it with a list of spans.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9EKFzmauifX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "# The words and spaces to create the doc from\n",
        "words = [\"Hello\", \"world\", \"!\"]\n",
        "spaces = [True, False, False]\n",
        "\n",
        "# Create a doc manually\n",
        "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
        "\n",
        "# Create a span manually\n",
        "span = Span(doc, 0, 2)\n",
        "\n",
        "# Create a span with a label\n",
        "span_with_label = Span(doc, 0, 2, label=\"GREETING\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQN1qbSy7c7H",
        "colab_type": "code",
        "outputId": "db980d3d-4243-4b1c-af60-cceef16ee659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for token in span_with_label:\n",
        "    print (token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zex3dhP_8ZES",
        "colab_type": "text"
      },
      "source": [
        "The text \"Hello world!\" is like a GREETING (entity). So we add it to the doc entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es1zuTdT7hdR",
        "colab_type": "code",
        "outputId": "4921f0f0-7e98-4b95-d960-f9c5cfe969c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Add span to the doc.ents\n",
        "doc.ents = [span_with_label]\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print (ent.text, ent.label_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world GREETING\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n2Tmov_8YBC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAG0FuOZ8kLc",
        "colab_type": "text"
      },
      "source": [
        "**Best practices**\n",
        "\n",
        "The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
        "\n",
        "If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
        "\n",
        "To keep things consistent, try to use built-in token attributes wherever possible. For example, token.i for the token index.\n",
        "\n",
        "Also, don't forget to always pass in the shared vocab!\n",
        "\n",
        "#### Exercises\n",
        "\n",
        "In this exercise, you’ll create the Doc and Span objects manually, and update the named entities – just like spaCy does behind the scenes. A shared nlp object has already been created.\n",
        "\n",
        "- Import the Doc and Span classes from spacy.tokens.\n",
        "- Use the Doc class directly to create a doc from the words and spaces.\n",
        "- Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
        "- Overwrite the doc.ents with a list of one entity, the “David Bowie” span."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwRBKEZK7_C5",
        "colab_type": "code",
        "outputId": "3491f973-3b63-4acf-f116-117f7dae2f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nlp = English()\n",
        "\n",
        "# Import the Doc and Span classes\n",
        "from spacy.tokens import Doc, Span\n",
        "\n",
        "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
        "spaces = [True, True, True, False]\n",
        "\n",
        "# Create a doc from the words and spaces\n",
        "doc = Doc(nlp.vocab, words, spaces)\n",
        "print(doc.text)\n",
        "\n",
        "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
        "span = Span(doc, 2, 4, label=\"PERSON\")\n",
        "print(span.text, span.label_)\n",
        "\n",
        "# Add the span to the doc's entities\n",
        "doc.ents = [span]\n",
        "\n",
        "# Print entities' text and labels\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I like David Bowie\n",
            "David Bowie PERSON\n",
            "[('David Bowie', 'PERSON')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOI58dOICPLh",
        "colab_type": "text"
      },
      "source": [
        "**Data Structures Best Practices**\n",
        "\n",
        "The code in this example is trying to analyze a text and collect all proper nouns that are followed by a verb.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM87RlBUA0G0",
        "colab_type": "code",
        "outputId": "9b72cfa8-7632-444a-b51e-0a7eec6a2acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Berlin is a nice city\")\n",
        "\n",
        "# Get all tokens and part-of-speech tags\n",
        "token_texts = [token.text for token in doc]\n",
        "pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "print (pos_tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYY2m5EZCW72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, pos in enumerate(pos_tags):\n",
        "    # Check if the current token is a proper noun\n",
        "    if pos == 'PROPN':\n",
        "        # Check if the next token is a verb\n",
        "        if pos_tags[index+1] == 'VERB':\n",
        "            result = token_texts[index]\n",
        "            print(\"Found proper noun before a verb:\", result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYmsDZ_lCy-p",
        "colab_type": "code",
        "outputId": "fda3f252-5428-44cc-d90a-59bdc5f45037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "[token.tag_ for token in doc]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NNP', 'VBZ', 'DT', 'JJ', 'NN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jrjaDsPDUXk",
        "colab_type": "text"
      },
      "source": [
        "Why is the code bad?\n",
        "\n",
        "- It only uses lists of strings instead of native token attributes. This is often less efficient, and can't express complex relationships."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeFJz-tpDE_d",
        "colab_type": "code",
        "outputId": "957fbc48-e706-4529-c54b-0e0bde87c0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(token_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyns7VU3Dfvp",
        "colab_type": "text"
      },
      "source": [
        "- Rewrite the code to use the native token attributes instead of lists of token_texts and pos_tags.\n",
        "- Loop over each token in the doc and check the token.pos_ attribute.\n",
        "- Use doc[token.i + 1] to check for the next token and its .pos_ attribute.\n",
        "- If a proper noun before a verb is found, print its token.text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqALQHRwD2ng",
        "colab_type": "code",
        "outputId": "15397853-b72e-439b-a5e0-50f301a0844e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc = nlp(\"Berlin depicts nice city Tokyo\")\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\" and token.i+1 < len(doc):\n",
        "        if doc[token.i + 1].pos_ == \"VERB\":\n",
        "            print (\"Found proper noun before a verb:\", token.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found proper noun before a verb: Berlin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3IYHQmY1fKJ",
        "colab_type": "text"
      },
      "source": [
        "#### Word vectors and semantic similarity\n",
        "\n",
        "spaCy can compare two objects and predict how similar they are – for example, documents, spans or single tokens.\n",
        "\n",
        "The Doc, Token and Span objects have a .similarity method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
        "\n",
        "One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
        "\n",
        "For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\". You can find more details on this in the [models documentation](https://spacy.io/models).\n",
        "\n",
        "**Similarity examples (1)**\n",
        "\n",
        "Here's an example. Let's say we want to find out whether two documents are similar.\n",
        "\n",
        "First, we load the medium English model, \"en_core_web_md\".\n",
        "\n",
        "We can then create two doc objects and use the first doc's similarity method to compare it to the second.\n",
        "\n",
        "Here, a fairly high similarity score of 0.86 is predicted for \"I like fast food\" and \"I like pizza\".\n",
        "\n",
        "The same works for tokens.\n",
        "\n",
        "According to the word vectors, the tokens \"pizza\" and \"pasta\" are kind of similar, and receive a score of 0.7.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3aQ-eAPEcy6",
        "colab_type": "code",
        "outputId": "0c1c813b-21fe-42d9-9cee-31f94ce97dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import spacy\n",
        "# Load a larger model with vectors\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# Compare two documents\n",
        "doc1 = nlp(\"I like fast food\")\n",
        "doc2 = nlp(\"I like pizza\")\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8627204117787385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RPe50O63Izp",
        "colab_type": "code",
        "outputId": "6796e991-b976-41d5-8fe2-7944e3eea472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Compare two tokens\n",
        "doc = nlp(\"I like pizza and pasta\")\n",
        "token1 = doc[2]\n",
        "token2 = doc[4]\n",
        "print(token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7369546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVnhfyN72-A_",
        "colab_type": "text"
      },
      "source": [
        "**Similarity examples (2)**\n",
        "\n",
        "You can also use the similarity methods to compare different types of objects.\n",
        "\n",
        "For example, a document and a token.\n",
        "\n",
        "Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
        "\n",
        "Here's another example comparing a span – \"pizza and pasta\" – to a document about McDonalds.\n",
        "\n",
        "The score returned here is 0.61, so it's determined to be kind of similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob_vtQ6N2SES",
        "colab_type": "code",
        "outputId": "f045960c-068c-42a9-e726-f5fc8d0f8697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Compare a document with a token\n",
        "doc = nlp(\"I like pizza\")\n",
        "token = nlp(\"soap\")[0]\n",
        "\n",
        "print(doc.similarity(token))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.32531983166759537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W0Iwvo53YKU",
        "colab_type": "code",
        "outputId": "8bec5f15-36b0-4f79-dbd4-ebe0bf3b197f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Compare a span with a document\n",
        "span = nlp(\"I like pizza and pasta\")[2:5]\n",
        "doc = nlp(\"McDonalds sells burgers\")\n",
        "\n",
        "print(span.similarity(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6199092090831612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1kJM2Hq3irL",
        "colab_type": "text"
      },
      "source": [
        "**How does spaCy predict similarity?**\n",
        "\n",
        "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
        "\n",
        "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
        "\n",
        "Vectors can be added to spaCy's statistical models.\n",
        "\n",
        "By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.\n",
        "\n",
        "> Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.\n",
        "\n",
        "> That's also why you usually get more value out of shorter phrases with fewer irrelevant words, **as in that case the average will not be affected by the outliers too much**.\n",
        "\n",
        "- average seems to be a simple solution but we might need something a bit more sophisticated, like WMD or Doc2Vec\n",
        "\n",
        "**Word vectors in spaCy**\n",
        "\n",
        "To give you an idea of what those vectors look like, here's an example.\n",
        "\n",
        "First, we load the medium model again, which ships with word vectors.\n",
        "\n",
        "Next, we can process a text and look up a token's vector using the .vector attribute.\n",
        "\n",
        "The result is a *300-dimensional* vector of the word \"banana\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evaeh3373e4P",
        "colab_type": "code",
        "outputId": "9eca85af-d0f2-4a73-b100-dca2a0c01ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "doc = nlp(\"I have a banana\")\n",
        "# Access the vector via the token.vector attribute\n",
        "print (type(doc[3].vector))\n",
        "print (doc[3].vector.shape)\n",
        "print(doc[3].vector[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(300,)\n",
            "[ 0.20228  -0.076618  0.37032   0.032845 -0.41957   0.072069 -0.37476\n",
            "  0.05746  -0.012401  0.52949 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjkvL-M24n6Q",
        "colab_type": "text"
      },
      "source": [
        "**Similarity depends on the application context**\n",
        "\n",
        "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.\n",
        "\n",
        "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.\n",
        "\n",
        "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eZCOFdf4RHN",
        "colab_type": "code",
        "outputId": "667ac144-24c3-49d9-c485-bdc8f43e7e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc1 = nlp(\"I like cats\")\n",
        "doc2 = nlp(\"I hate cats\")\n",
        "\n",
        "print(doc1.similarity(doc2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9501447503553421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84aj7XSC4uRY",
        "colab_type": "code",
        "outputId": "230eb94a-7332-4055-ca2d-8fa6452d1115",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
        "\n",
        "doc[-4:-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "really nice bar"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7toGxc9i_E28",
        "colab_type": "text"
      },
      "source": [
        "#### Combining models and rules\n",
        "\n",
        "Combining statistical models with rule-based systems is one of the most powerful tricks you should have in your NLP toolbox.\n",
        "\n",
        "**Statistical predictions vs. rules**\n",
        "\n",
        "Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
        "\n",
        "For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.\n",
        "\n",
        "To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger.\n",
        "\n",
        "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.\n",
        "\n",
        "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher.\n",
        "\n",
        "![](https://i.ibb.co/rHmYNgQ/diag4.png)\n",
        "\n",
        "**Recap: Rule-based Matching**\n",
        "\n",
        "In the last chapter, you learned how to use spaCy's rule-based matcher to find complex patterns in your texts. Here's a quick recap.\n",
        "\n",
        "The matcher is initialized with the shared vocabulary – usually nlp.vocab.\n",
        "\n",
        "Patterns are lists of dictionaries, and each dictionary describes one token and its attributes. Patterns can be added to the matcher using the matcher.add method.\n",
        "\n",
        "Operators let you specify how often to match a token. For example, \"+\" will match one or more times.\n",
        "\n",
        "Calling the matcher on a doc object will return a list of the matches. Each match is a tuple consisting of an ID, and the start and end token index in the document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iim-vn026ccJ",
        "colab_type": "code",
        "outputId": "895b1508-7c55-44b1-d73c-39f52b2c1343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Patterns are lists of dictionaries describing the tokens\n",
        "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
        "matcher.add(\"LOVE_CATS\", None, pattern)\n",
        "\n",
        "# Operators can specify how often a token should be matched\n",
        "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
        "matcher.add(\"VERY_HAPPY\", None, pattern)\n",
        "\n",
        "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
        "doc = nlp(\"I love cats and I'm very very happy\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "print (matches)\n",
        "\n",
        "# Iterate over the matches and print the span text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Match found:\", doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(9137535031263442622, 1, 3), (2447047934687575526, 7, 9), (2447047934687575526, 6, 9)]\n",
            "Match found: love cats\n",
            "Match found: very happy\n",
            "Match found: very very happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a8G_AIBDt-w",
        "colab_type": "text"
      },
      "source": [
        "**Adding statistical predictions**\n",
        "\n",
        "Here's an example of a matcher rule for \"golden retriever\".\n",
        "\n",
        "If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span. We can then find out more about it. **Span objects give us access to the original document and all other token attributes and linguistic features predicted by the model**.\n",
        "\n",
        "For example, we can get the span's root token. If the span consists of more than one token, this will be the *token that decides the category of the phrase*. For example, the root of \"Golden Retriever\" is \"Retriever\". We can also find the head token of the root. This is the syntactic \"parent\" that governs the phrase – in this case, the verb \"have\".\n",
        "\n",
        "Finally, we can look at the previous token and its attributes. In this case, it's a determiner, the article \"a\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyC6ne7ODZJQ",
        "colab_type": "code",
        "outputId": "12604902-45f5-4ee8-f4ac-5b22de9a320a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"DOG\", None, [{\"LOWER\": \"golden\"}, {\"LOWER\": \"retriever\"}])\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # get the matched span\n",
        "    span = doc[start:end]\n",
        "    # Get the span's root token and root head token\n",
        "    print(\"Root token:\", span.root.text)\n",
        "    print(\"Root head token:\", span.root.head.text)\n",
        "    # Get the previous token and its POS tag\n",
        "    print (f'Previous token is \"{doc[start-1].text}\" and it is a \"{doc[start-1].pos_}\"')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root token: Retriever\n",
            "Root head token: have\n",
            "Previous token is \"a\" and it is a \"DET\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCOM4oK4FGom",
        "colab_type": "text"
      },
      "source": [
        "**Efficient phrase matching**\n",
        "\n",
        "The phrase matcher is another helpful tool to find sequences of words in your data.\n",
        "\n",
        "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
        "\n",
        "It takes Doc objects as patterns.\n",
        "\n",
        "It's also really fast.\n",
        "\n",
        "This makes it very useful for matching large dictionaries and word lists on large volumes of text.\n",
        "\n",
        "The phrase matcher can be imported from spacy.matcher and follows the same API as the regular matcher.\n",
        "\n",
        "Instead of a list of dictionaries, we pass in a Doc object as the pattern.\n",
        "\n",
        "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihJEvl8xE3vk",
        "colab_type": "code",
        "outputId": "19d6e508-7c0c-488c-daaa-50efcfd4732d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "pattern = nlp(\"Golden Retriever\")\n",
        "matcher.add(\"DOG\", None, pattern)\n",
        "doc = nlp(\"I have a Golden Retriever\")\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Get the matched span\n",
        "    span = doc[start:end]\n",
        "    print(\"Matched span:\", span.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matched span: Golden Retriever\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8F8Hfz8FrK-",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "**Debugging patterns (1)**\n",
        "\n",
        "Why does this pattern not match the tokens “Silicon Valley” in the doc?\n",
        "\n",
        "```\n",
        "pattern = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\": \"valley\"}]\n",
        "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
        "```\n",
        "- The tokenizer doesn’t create tokens for single spaces, so there’s no token with the value \" \" in between.\n",
        "\n",
        "**Debugging patterns (2)**\n",
        "\n",
        "\n",
        "Both patterns in this exercise contain mistakes and won’t match as expected. Can you fix them? If you get stuck, try printing the tokens in the doc to see how the text will be split and adjust the pattern so that each dictionary represents one token.\n",
        "\n",
        "- Edit pattern1 so that it correctly matches all case-insensitive mentions of \"Amazon\" plus a title-cased proper noun.\n",
        "- Edit pattern2 so that it correctly matches all case-insensitive mentions of \"ad-free\", plus the following noun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxZB350LFa0t",
        "colab_type": "code",
        "outputId": "ab3a1d7b-3071-4079-acca-2122c729f81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\n",
        "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
        "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
        "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
        "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
        "    \"Prime for new members, beginning on September 14. However, members with \"\n",
        "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
        "    \"viewing until their subscription comes up for renewal. Those with \"\n",
        "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
        ")\n",
        "\n",
        "\n",
        "# Create the match patterns\n",
        "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
        "pattern2 = [{\"LOWER\": \"ad\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
        "\n",
        "# Initialize the Matcher and add the patterns\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"PATTERN1\", None, pattern1)\n",
        "matcher.add(\"PATTERN2\", None, pattern2)\n",
        "\n",
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Print pattern string name and text of matched span\n",
        "    print(doc.vocab.strings[match_id], doc[start:end].text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PATTERN1 Amazon Prime\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN1 Amazon Prime\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN2 ad-free viewing\n",
            "PATTERN2 ad-free viewing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNSN2Ni9WM5j",
        "colab_type": "text"
      },
      "source": [
        "For the token '-', you can match on the attribute 'TEXT',\n",
        "'LOWER' or even 'SHAPE'. All of those are correct. As you can see, paying close\n",
        "attention to the tokenization is very important when working with the\n",
        "token-based 'Matcher'. Sometimes it's much easier to just match exact strings\n",
        "instead and use the 'PhraseMatcher', which we'll get to in the next\n",
        "exercise.\n",
        "\n",
        "\n",
        "**Efficient Phrase Matching**\n",
        "\n",
        "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES.\n",
        "\n",
        "- Import the PhraseMatcher and initialize it with the shared vocab as the variable matcher.\n",
        "- Add the phrase patterns and call the matcher on the doc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndub8vh6VUiJ",
        "colab_type": "code",
        "outputId": "235880b7-a77d-4ac2-ee5c-5c8faca76f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import json\n",
        "\n",
        "with open('./countries.json') as f:\n",
        "    COUNTRIES = json.loads(f.read())\n",
        "\n",
        "print (COUNTRIES)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'country': 'Afghanistan'}, {'country': 'Albania'}, {'country': 'Algeria'}, {'country': 'American Samoa'}, {'country': 'Andorra'}, {'country': 'Angola'}, {'country': 'Anguilla'}, {'country': 'Antarctica'}, {'country': 'Antigua and Barbuda'}, {'country': 'Argentina'}, {'country': 'Armenia'}, {'country': 'Aruba'}, {'country': 'Australia'}, {'country': 'Austria'}, {'country': 'Azerbaijan'}, {'country': 'Bahamas'}, {'country': 'Bahrain'}, {'country': 'Bangladesh'}, {'country': 'Barbados'}, {'country': 'Belarus'}, {'country': 'Belgium'}, {'country': 'Belize'}, {'country': 'Benin'}, {'country': 'Bermuda'}, {'country': 'Bhutan'}, {'country': 'Bolivia'}, {'country': 'Bosnia and Herzegovina'}, {'country': 'Botswana'}, {'country': 'Bouvet Island'}, {'country': 'Brazil'}, {'country': 'British Indian Ocean Territory'}, {'country': 'Brunei'}, {'country': 'Bulgaria'}, {'country': 'Burkina Faso'}, {'country': 'Burundi'}, {'country': 'Cambodia'}, {'country': 'Cameroon'}, {'country': 'Canada'}, {'country': 'Cape Verde'}, {'country': 'Cayman Islands'}, {'country': 'Central African Republic'}, {'country': 'Chad'}, {'country': 'Chile'}, {'country': 'China'}, {'country': 'Christmas Island'}, {'country': 'Cocos (Keeling) Islands'}, {'country': 'Colombia'}, {'country': 'Comoros'}, {'country': 'Congo'}, {'country': 'The Democratic Republic of Congo'}, {'country': 'Cook Islands'}, {'country': 'Costa Rica'}, {'country': 'Ivory Coast'}, {'country': 'Croatia'}, {'country': 'Cuba'}, {'country': 'Cyprus'}, {'country': 'Czech Republic'}, {'country': 'Denmark'}, {'country': 'Djibouti'}, {'country': 'Dominica'}, {'country': 'Dominican Republic'}, {'country': 'East Timor'}, {'country': 'Ecuador'}, {'country': 'Egypt'}, {'country': 'England'}, {'country': 'El Salvador'}, {'country': 'Equatorial Guinea'}, {'country': 'Eritrea'}, {'country': 'Estonia'}, {'country': 'Ethiopia'}, {'country': 'Falkland Islands'}, {'country': 'Faroe Islands'}, {'country': 'Fiji Islands'}, {'country': 'Finland'}, {'country': 'France'}, {'country': 'French Guiana'}, {'country': 'French Polynesia'}, {'country': 'French Southern territories'}, {'country': 'Gabon'}, {'country': 'Gambia'}, {'country': 'Georgia'}, {'country': 'Germany'}, {'country': 'Ghana'}, {'country': 'Gibraltar'}, {'country': 'Greece'}, {'country': 'Greenland'}, {'country': 'Grenada'}, {'country': 'Guadeloupe'}, {'country': 'Guam'}, {'country': 'Guatemala'}, {'country': 'Guernsey'}, {'country': 'Guinea'}, {'country': 'Guinea-Bissau'}, {'country': 'Guyana'}, {'country': 'Haiti'}, {'country': 'Heard Island and McDonald Islands'}, {'country': 'Holy See (Vatican City State)'}, {'country': 'Honduras'}, {'country': 'Hong Kong'}, {'country': 'Hungary'}, {'country': 'Iceland'}, {'country': 'India'}, {'country': 'Indonesia'}, {'country': 'Iran'}, {'country': 'Iraq'}, {'country': 'Ireland'}, {'country': 'Israel'}, {'country': 'Isle of Man'}, {'country': 'Italy'}, {'country': 'Jamaica'}, {'country': 'Japan'}, {'country': 'Jersey'}, {'country': 'Jordan'}, {'country': 'Kazakhstan'}, {'country': 'Kenya'}, {'country': 'Kiribati'}, {'country': 'Kuwait'}, {'country': 'Kyrgyzstan'}, {'country': 'Laos'}, {'country': 'Latvia'}, {'country': 'Lebanon'}, {'country': 'Lesotho'}, {'country': 'Liberia'}, {'country': 'Libyan Arab Jamahiriya'}, {'country': 'Liechtenstein'}, {'country': 'Lithuania'}, {'country': 'Luxembourg'}, {'country': 'Macao'}, {'country': 'North Macedonia'}, {'country': 'Madagascar'}, {'country': 'Malawi'}, {'country': 'Malaysia'}, {'country': 'Maldives'}, {'country': 'Mali'}, {'country': 'Malta'}, {'country': 'Marshall Islands'}, {'country': 'Martinique'}, {'country': 'Mauritania'}, {'country': 'Mauritius'}, {'country': 'Mayotte'}, {'country': 'Mexico'}, {'country': 'Micronesia, Federated States of'}, {'country': 'Moldova'}, {'country': 'Monaco'}, {'country': 'Mongolia'}, {'country': 'Montserrat'}, {'country': 'Montenegro'}, {'country': 'Morocco'}, {'country': 'Mozambique'}, {'country': 'Myanmar'}, {'country': 'Namibia'}, {'country': 'Nauru'}, {'country': 'Nepal'}, {'country': 'Netherlands'}, {'country': 'Netherlands Antilles'}, {'country': 'New Caledonia'}, {'country': 'New Zealand'}, {'country': 'Nicaragua'}, {'country': 'Niger'}, {'country': 'Nigeria'}, {'country': 'Niue'}, {'country': 'Norfolk Island'}, {'country': 'North Korea'}, {'country': 'Northern Ireland'}, {'country': 'Northern Mariana Islands'}, {'country': 'Norway'}, {'country': 'Oman'}, {'country': 'Pakistan'}, {'country': 'Palau'}, {'country': 'Palestine'}, {'country': 'Panama'}, {'country': 'Papua New Guinea'}, {'country': 'Paraguay'}, {'country': 'Peru'}, {'country': 'Philippines'}, {'country': 'Pitcairn'}, {'country': 'Poland'}, {'country': 'Portugal'}, {'country': 'Puerto Rico'}, {'country': 'Qatar'}, {'country': 'Reunion'}, {'country': 'Romania'}, {'country': 'Russian Federation'}, {'country': 'Rwanda'}, {'country': 'Saint Helena'}, {'country': 'Saint Kitts and Nevis'}, {'country': 'Saint Lucia'}, {'country': 'Saint Pierre and Miquelon'}, {'country': 'Saint Vincent and the Grenadines'}, {'country': 'Samoa'}, {'country': 'San Marino'}, {'country': 'Sao Tome and Principe'}, {'country': 'Saudi Arabia'}, {'country': 'Scotland'}, {'country': 'Senegal'}, {'country': 'Serbia'}, {'country': 'Seychelles'}, {'country': 'Sierra Leone'}, {'country': 'Singapore'}, {'country': 'Slovakia'}, {'country': 'Slovenia'}, {'country': 'Solomon Islands'}, {'country': 'Somalia'}, {'country': 'South Africa'}, {'country': 'South Georgia and the South Sandwich Islands'}, {'country': 'South Korea'}, {'country': 'South Sudan'}, {'country': 'Spain'}, {'country': 'SriLanka'}, {'country': 'Sudan'}, {'country': 'Suriname'}, {'country': 'Svalbard and Jan Mayen'}, {'country': 'Swaziland'}, {'country': 'Sweden'}, {'country': 'Switzerland'}, {'country': 'Syria'}, {'country': 'Tajikistan'}, {'country': 'Tanzania'}, {'country': 'Thailand'}, {'country': 'Timor-Leste'}, {'country': 'Togo'}, {'country': 'Tokelau'}, {'country': 'Tonga'}, {'country': 'Trinidad and Tobago'}, {'country': 'Tunisia'}, {'country': 'Turkey'}, {'country': 'Turkmenistan'}, {'country': 'Turks and Caicos Islands'}, {'country': 'Tuvalu'}, {'country': 'Uganda'}, {'country': 'Ukraine'}, {'country': 'United Arab Emirates'}, {'country': 'United Kingdom'}, {'country': 'United States'}, {'country': 'United States Minor Outlying Islands'}, {'country': 'Uruguay'}, {'country': 'Uzbekistan'}, {'country': 'Vanuatu'}, {'country': 'Venezuela'}, {'country': 'Vietnam'}, {'country': 'Virgin Islands, British'}, {'country': 'Virgin Islands, U.S.'}, {'country': 'Wales'}, {'country': 'Wallis and Futuna'}, {'country': 'Western Sahara'}, {'country': 'Yemen'}, {'country': 'Yugoslavia'}, {'country': 'Zambia'}, {'country': 'Zimbabwe'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI_VIFjkZlcF",
        "colab_type": "code",
        "outputId": "79814e1e-47f8-42ee-d24b-cc2b8399422a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "COUNTRIES = [country['country'] for country in COUNTRIES]\n",
        "\n",
        "print (COUNTRIES)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Afghanistan', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'The Democratic Republic of Congo', 'Cook Islands', 'Costa Rica', 'Ivory Coast', 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'England', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands', 'Faroe Islands', 'Fiji Islands', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See (Vatican City State)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Isle of Man', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libyan Arab Jamahiriya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'North Macedonia', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova', 'Monaco', 'Mongolia', 'Montserrat', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'Netherlands Antilles', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'North Korea', 'Northern Ireland', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Reunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Helena', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Scotland', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'South Korea', 'South Sudan', 'Spain', 'SriLanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Tajikistan', 'Tanzania', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela', 'Vietnam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wales', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Yugoslavia', 'Zambia', 'Zimbabwe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejeg8pBiZuJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
        "\n",
        "# Import the PhraseMatcher and initialize it\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2rkTe4maE2z",
        "colab_type": "code",
        "outputId": "189dc3c2-982b-4296-d39a-20eb6297f672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create pattern Doc objects and add them to the matcher\n",
        "# sample pattern : pattern = nlp(\"Golden Retriever\")\n",
        "patterns = [nlp(country_) for country_ in COUNTRIES]\n",
        "print (len(patterns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLUOgeLObpIT",
        "colab_type": "code",
        "outputId": "98afdcf8-05bd-43ab-f5e9-e376cd106994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
        "patterns = list(nlp.pipe(COUNTRIES))\n",
        "print (len(patterns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX9ZVj07btgF",
        "colab_type": "code",
        "outputId": "ab27fda6-36cb-47b2-8cae-9b38bcac2e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "matcher.add(\"COUNTRY\", None, *patterns)\n",
        "# Call the matcher on the test document and print the result\n",
        "matches = matcher(doc)\n",
        "print([doc[start:end] for match_id, start, end in matches])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Czech Republic, Slovakia]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzwi93RPc7iJ",
        "colab_type": "text"
      },
      "source": [
        "**Extracting countries and relationships**\n",
        "\n",
        "In the previous exercise, you wrote a script using spaCy’s PhraseMatcher to find country names in text. Let’s use that country matcher on a longer text, analyze the syntax and update the document’s entities with the matched countries.\n",
        "\n",
        "- Iterate over the matches and create a Span with the label \"GPE\" (geopolitical entity).\n",
        "- Overwrite the entities in doc.ents and add the matched span.\n",
        "- Get the matched span’s root head token.\n",
        "- Print the text of the head token and the span."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojpZMXUscE3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZgQWPNodQJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = \"\"\"\n",
        "After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in ten years than it had in the previous four decades.\n",
        "Between 1988 and 2000, the number of adopted Security Council resolutions more than doubled, and the peacekeeping budget increased more than tenfold. \n",
        "The UN negotiated an end to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic elections in post-apartheid \n",
        "South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led coalition that repulsed the Iraqi invasion of Kuwait. \n",
        "Brian Urquhart, Under-Secretary-General from 1971 to 1985, later described the hopes raised by these successes as a \"false renaissance\" for the organization, \n",
        "given the more troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one nation against another, \n",
        "in the early 1990s the UN faced a number of simultaneous, serious crises within nations such as Somalia, Haiti, Mozambique, and the former Yugoslavia. \n",
        "The UN mission in Somalia was widely viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission \n",
        "to Bosnia faced \"worldwide ridicule\" for its indecisive and confused mission in the face of ethnic cleansing. In 1994, the UN Assistance Mission for \n",
        "Rwanda failed to intervene in the Rwandan genocide amid indecision in the Security Council. Beginning in the last decades of the Cold War, American and \n",
        "European critics of the UN condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, withdrew \n",
        "his nation's funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, founded 1946) over allegations of mismanagement, \n",
        "followed by Britain and Singapore. Boutros Boutros-Ghali, Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of \n",
        "the organization somewhat. His successor, Kofi Annan (1997–2006), initiated further management reforms in the face of threats from the United States to \n",
        "withhold its UN dues. In the late 1990s and 2000s, international interventions authorized by the UN took a wider variety of forms. The UN mission in the \n",
        "Sierra Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 was overseen by NATO. \n",
        "In 2003, the United States invaded Iraq despite failing to pass a UN Security Council resolution for authorization, prompting a new round of questioning of \n",
        "the organization's effectiveness. Under the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the \n",
        "War in Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons inspectors to the Syrian Civil War. \n",
        "In 2013, an internal review of UN actions in the final battles of the Sri Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". \n",
        "One hundred and one UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization's history. The Millennium Summit was held in 2000 \n",
        "to discuss the UN's role in the 21st century. The three day meeting was the largest gathering of world leaders in history, and culminated in the adoption by all \n",
        "member states of the Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty reduction, gender equality, \n",
        "and public health. Progress towards these goals, which were to be met by 2015, was ultimately uneven. The 2005 World Summit reaffirmed the UN's focus on \n",
        "promoting development, peacekeeping, human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the Millennium Development Goals. \n",
        "In addition to addressing global challenges, the UN has sought to improve its accountability and democratic legitimacy by engaging more with civil society \n",
        "and fostering a global constituency. In an effort to enhance transparency, in 2016 the organization held its first public debate between candidates for Secretary-General. \n",
        "On 1 January 2017, Portuguese diplomat António Guterres, who previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. \n",
        "Guterres has highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, more effective peacekeeping efforts, \n",
        "and streamlining the organization to be more responsive and versatile to global needs.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwvd1HObfVDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "patterns = list(nlp.pipe(COUNTRIES))\n",
        "matcher.add(\"COUNTRY\", None, *patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BC3JiH_eBtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a doc and reset existing entities\n",
        "doc = nlp(TEXT)\n",
        "doc.ents = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdQ-IOe3eCTu",
        "colab_type": "code",
        "outputId": "db983a84-132f-401c-8a68-ab7de5edbe63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "# Iterate over the matches\n",
        "for match_id, start, end in matcher(doc):\n",
        "    # Create a Span with the label for \"GPE\"\n",
        "    span = Span(doc, start, end, label=\"GPE\")\n",
        "\n",
        "    # Overwrite the doc.ents and add the span\n",
        "    doc.ents = list(doc.ents) + [span]\n",
        "\n",
        "    # Get the span's root head token\n",
        "    span_root_head = span.root.head\n",
        "    # Print the text of the span root's head token and the span text\n",
        "    print(span_root_head.text, \"-->\", span.text)\n",
        "\n",
        "# Print the entities in the document\n",
        "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "in --> Namibia\n",
            "in --> South Africa\n",
            "Africa --> Cambodia\n",
            "of --> Kuwait\n",
            "as --> Somalia\n",
            "Somalia --> Haiti\n",
            "Haiti --> Mozambique\n",
            "Mozambique --> Yugoslavia\n",
            "in --> Somalia\n",
            "failed --> Rwanda\n",
            "Britain --> Singapore\n",
            "from --> United States\n",
            "War --> Sierra Leone\n",
            "of --> Afghanistan\n",
            "invaded --> United States\n",
            "invaded --> Iraq\n",
            "in --> Sudan\n",
            "of --> Congo\n",
            "earthquake --> Haiti\n",
            "[('Namibia', 'GPE'), ('South Africa', 'GPE'), ('Cambodia', 'GPE'), ('Kuwait', 'GPE'), ('Somalia', 'GPE'), ('Haiti', 'GPE'), ('Mozambique', 'GPE'), ('Yugoslavia', 'GPE'), ('Somalia', 'GPE'), ('Rwanda', 'GPE'), ('Singapore', 'GPE'), ('United States', 'GPE'), ('Sierra Leone', 'GPE'), ('Afghanistan', 'GPE'), ('United States', 'GPE'), ('Iraq', 'GPE'), ('Sudan', 'GPE'), ('Congo', 'GPE'), ('Haiti', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Mv4gCJmOE_",
        "colab_type": "text"
      },
      "source": [
        "### Chapter 3: Processing Pipelines\n",
        "\n",
        "This chapter will show you everything you need to know about spaCy's processing pipeline. You'll learn what goes on under the hood when you process a text, how to write your own components and add them to the pipeline, and how to use custom attributes to add your own metadata to the documents, spans and tokens.\n",
        "\n",
        "#### Processing pipelines\n",
        "\n",
        "**What happens when you call nlp?**\n",
        "\n",
        "![](https://course.spacy.io/pipeline.png)\n",
        "\n",
        "You've already written this plenty of times by now: pass a string of text to the nlp object, and receive a Doc object.\n",
        "\n",
        "But what does the nlp object actually do?\n",
        "\n",
        "\n",
        "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed doc is returned, so you can work with it.\n",
        "\n",
        "**Built-in pipeline components**\n",
        "\n",
        "![](https://i.ibb.co/kxyG66N/diag5.png)\n",
        "\n",
        "spaCy ships with the following built-in pipeline components.\n",
        "\n",
        "The part-of-speech tagger sets the token.tag and token.pos attributes.\n",
        "\n",
        "The dependency parser adds the token.dep and token.head attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
        "\n",
        "The named entity recognizer adds the detected entities to the doc.ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
        "\n",
        "Finally, the text classifier sets category labels that apply to the whole text, and adds them to the doc.cats property.\n",
        "\n",
        "> Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system.\n",
        "\n",
        "**Under the hood**\n",
        "\n",
        "![](https://course.spacy.io/package_meta.png)\n",
        "\n",
        "All models you can load into spaCy include several files and a `meta.json`.\n",
        "\n",
        "The meta defines things like the language and pipeline. This tells spaCy which components to instantiate.\n",
        "\n",
        "The built-in components that make predictions also need binary data. The data is included in the model package and loaded into the component when you load the model.\n",
        "\n",
        "**Pipeline attributes**\n",
        "\n",
        "To see the names of the pipeline components present in the current nlp object, you can use the `nlp.pipe_names` attribute.\n",
        "\n",
        "For a list of component name and component function tuples, you can use the `nlp.pipeline` attribute.\n",
        "\n",
        "The component functions are the functions applied to the doc to process it and set attributes – for example, part-of-speech tags or named entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbfzLlxdfYmH",
        "colab_type": "code",
        "outputId": "0bce7a34-6afe-4930-f6fa-0744f3ab4c0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print (nlp.pipe_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kogPLdkbuI3F",
        "colab_type": "code",
        "outputId": "49310414-73f6-4c7a-cb95-1cd4417a79bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(nlp.pipeline)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f0b84c53710>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f0b84f71648>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f0b84f716a8>)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzC7x6cwuUEl",
        "colab_type": "text"
      },
      "source": [
        "#### Excercises\n",
        "\n",
        "What does spaCy do when you call nlp on a string of text?\n",
        "\n",
        "`doc = nlp(\"This is a sentence.\")`\n",
        "\n",
        "- Tokenize the text and apply each pipeline component in order.\n",
        "\n",
        "#### Custom pipeline components\n",
        "\n",
        "Custom pipeline components let you add your own function to the spaCy pipeline that is executed when you call the nlp object on a text – for example, to modify the doc and add more data to it.\n",
        "\n",
        "**Why custom components?**\n",
        "\n",
        "![](https://course.spacy.io/pipeline.png)\n",
        "\n",
        "After the text is tokenized and a Doc object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own.\n",
        "\n",
        "Custom components are executed automatically when you call the nlp object on a text.\n",
        "\n",
        "They're especially useful for adding your own custom metadata to documents and tokens.\n",
        "\n",
        "You can also use them to update built-in attributes, like the named entity spans.\n",
        "\n",
        "**Anatomy of a component**\n",
        "\n",
        "Fundamentally, a pipeline component is a function or callable that **takes a doc**, modifies it and returns the **modified doc**, so it can be processed by the next component in the pipeline.\n",
        "\n",
        "Components can be added to the pipeline using the `nlp.add_pipe` method. The method takes at least one argument: the component function.\n",
        "\n",
        "```\n",
        "def custom_component(doc):\n",
        "    # Do something to the doc here\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(custom_component)\n",
        "```\n",
        "\n",
        "![](https://i.ibb.co/GRfW9NX/diag6.png)\n",
        "\n",
        "**Example: a simple component**\n",
        "\n",
        "Here's an example of a simple pipeline component.\n",
        "\n",
        "We start off with the small English model.\n",
        "\n",
        "We then define the component – a function that takes a Doc object and returns it.\n",
        "\n",
        "Let's do something simple and print the length of the doc that passes through the pipeline.\n",
        "\n",
        "Don't forget to return the doc so it can be processed by the next component in the pipeline! The doc created by the tokenizer is passed through all components, so it's important that they all return the modified doc.\n",
        "\n",
        "We can now add the component to the pipeline. Let's add it to the very beginning right after the tokenizer by setting first=True.\n",
        "\n",
        "When we print the pipeline component names, the custom component now shows up at the start. This means it will be applied first when we process a doc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUb97nXBuLQt",
        "colab_type": "code",
        "outputId": "5dc5a015-d2c9-4100-e7b6-e4ef09935cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a custom component\n",
        "def custom_component(doc):\n",
        "    # print the length of the doc\n",
        "    print (f'Length of the doc is {len(doc)}')\n",
        "    # return the modified doc\n",
        "    return doc\n",
        "\n",
        "## add the component first in the pipeline (just after the Tokenizer)\n",
        "nlp.add_pipe(custom_component, first=True)\n",
        "# Print the pipeline component names\n",
        "print(\"Pipeline:\", nlp.pipe_names)\n",
        "\n",
        "# Now when we process a text using the nlp object, the custom component will be applied to the doc and the length of the document will be printed.\n",
        "doc = nlp(\"Hello world!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n",
            "Length of the doc is 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UEaTSxVD62o",
        "colab_type": "text"
      },
      "source": [
        "#### Exercises\n",
        "\n",
        "Which of these problems can be solved by custom pipeline components? Choose all that apply!\n",
        "\n",
        "1. Updating the pre-trained models and improving their predictions\n",
        "2. Computing your own values based on tokens and their attributes\n",
        "3. Adding named entities, for example based on a dictionary\n",
        "4. Implementing support for an additional language\n",
        "\n",
        "- 2 and 3\n",
        "\n",
        "Custom components are great for adding custom values to documents, tokens and spans, and customizing the doc.ents.\n",
        "\n",
        "**Complex components**\n",
        "\n",
        "In this exercise, you’ll be writing a custom component that uses the `PhraseMatcher` to find animal names in the document and adds the matched spans to the `doc.ents`. A `PhraseMatcher` with the animal patterns has already been created as the variable matcher.\n",
        "\n",
        "- Define the custom component and apply the matcher to the doc.\n",
        "- Create a Span for each match, assign the label ID for \"ANIMAL\" and overwrite the doc.ents with the new spans.\n",
        "- Add the new component to the pipeline after the \"ner\" component.\n",
        "- Process the text and print the entity text and entity label for the entities in doc.ents.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z13WLRIkDW8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.tokens import Span"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCwoCT4FGPfu",
        "colab_type": "code",
        "outputId": "e7f74e28-c5a4-4387-c6a0-120c9a30409c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
        "animal_patterns = list(nlp.pipe(animals))\n",
        "\n",
        "print (animal_patterns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Golden Retriever, cat, turtle, Rattus norvegicus]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O6d6MeCGb6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init the matcher\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "# add the patterns to the matcher\n",
        "matcher.add(\"ANIMAL\", None, *animal_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdbo8fx8Gspq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the custom component\n",
        "def animal_component(doc):\n",
        "    # Apply the matcher to the doc\n",
        "    matches = matcher(doc)\n",
        "\n",
        "    # Create a Span for each match and assign the label \"ANIMAL\": span = Span(doc, start, end, label=\"GPE\")\n",
        "    spans = [Span(doc, start, end, \"ANIMAL\") for match_id, start, end in matches]\n",
        "\n",
        "    # Overwrite the doc.ents with the matched spans\n",
        "    doc.ents = spans\n",
        "    return doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UpOIRFTH9Zs",
        "colab_type": "code",
        "outputId": "a7412af2-46b2-4982-c8a2-f0541f972591",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Add the component to the pipeline after the \"ner\" component\n",
        "nlp.add_pipe(animal_component, after='ner')\n",
        "print(nlp.pipe_names)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tagger', 'parser', 'ner', 'animal_component']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwx4uttiIObm",
        "colab_type": "code",
        "outputId": "a0c4b8be-2054-44ac-9a28-96335add79cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Process the text and print the text and label for the doc.ents\n",
        "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
        "\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i_RSaAMgPp0",
        "colab_type": "text"
      },
      "source": [
        "#### Extension attributes\n",
        "\n",
        "These let us add **custom attributes** to our Doc, Span and Tokens and offer a wide range of flexibility\n",
        "\n",
        "**Setting custom attributes**\n",
        "\n",
        "Custom attributes let you add any metadata to docs, tokens and spans. The data can be added once, or it can be computed dynamically.\n",
        "\n",
        "Custom attributes are available via the `._` (dot underscore) property. This makes it clear that they were added by the user, and not built into spaCy, like `token.text`.\n",
        "\n",
        "Attributes need to be registered on the global Doc, Token and Span classes you can import from spacy.tokens. You've already worked with those in the previous chapters. To register a custom attribute on the Doc, Token and Span, you can use the set_extension method.\n",
        "\n",
        "The first argument is the attribute name. Keyword arguments let you define how the value should be computed. In this case, it has a default value and can be overwritten.\n",
        "\n",
        "- first set the attributes for the Doc/Span/Token\n",
        "- modify and retrieve the attributes\n",
        "\n",
        "\n",
        "**Options to set the extension**\n",
        "```\n",
        "name (unicode): Name of the attribute to set.\n",
        "default: Optional default value of the attribute.\n",
        "getter (callable): Optional getter function.\n",
        "setter (callable): Optional setter function.\n",
        "method (callable): Optional method for method extension.\n",
        "force (bool): Force overwriting existing attribute.\n",
        "```\n",
        "\n",
        "More on these on the types of extensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhehsj6vIXCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b1fd89e-52c6-4d0e-d42f-43eb44e6408a"
      },
      "source": [
        "doc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I have a cat and a Golden Retriever"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf0EjGT3g9Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.tokens import Doc, Span, Token\n",
        "\n",
        "# Set extensions on the Doc, Token and Span\n",
        "Doc.set_extension(name=\"title\", default=None)\n",
        "Token.set_extension(name=\"is_color\", default=False)\n",
        "Span.set_extension(name=\"has_color\", default=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxpvnCvUhGWI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "970d507b-cd0e-4d3a-8fc1-740e70818872"
      },
      "source": [
        "doc._.title = \"My pets\"\n",
        "print (doc._.title)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My pets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEB-4-6qnVTG",
        "colab_type": "text"
      },
      "source": [
        "#### Extension attribute types\n",
        "\n",
        "1. Attribute extensions\n",
        "2. Property extensions\n",
        "3. Method extensions\n",
        "\n",
        "#### Attribute extensions\n",
        "\n",
        "Attribute extensions set a default value that can be overwritten.\n",
        "\n",
        "For example, a custom `is_color` attribute on the token that defaults to False.\n",
        "\n",
        "On individual tokens, its value can be changed by overwriting it – in this case, True for the token \"blue\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrdW8lium_mN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "689287b6-03c7-48b8-8edf-94503b047987"
      },
      "source": [
        "doc = nlp(\"The sky is blue\")\n",
        "print (f'The property is_color for token: {doc[3]} is: {doc[3]._.is_color}')\n",
        "doc[3]._.is_color = True\n",
        "print (f'The property is_color for token: {doc[3]} is: {doc[3]._.is_color}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The property is_color for token: blue is: False\n",
            "The property is_color for token: blue is: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3gbS9FArhcc",
        "colab_type": "text"
      },
      "source": [
        "**Property extensions**\n",
        "\n",
        "Property extensions work like properties in Python: they can define a getter function and an optional setter.\n",
        "\n",
        "When we set the extension we have to specify the getter function. (in the following code: `Token.set_extension(\"is_color\", getter=get_is_color)`)\n",
        "\n",
        "The getter function gets called when we retrieve the property in `doc[3]._.is_color`\n",
        "\n",
        "Getter functions take one argument: the object (doc/span/token), in this case, the token. In this example, the function returns whether the token text is in our list of colors.\n",
        "\n",
        "\n",
        "NOTE: Previously we had created an attribute `is_color` on the Token, so to overwrite it, we have to set `force=True` on `Token.set_extension`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ULH0jfsnK2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "543a3cef-672d-4bf9-89e3-f5b229758cc2"
      },
      "source": [
        "# define the getter function\n",
        "def get_is_color(token):\n",
        "    colors = [\"red\", \"green\", \"blue\"]\n",
        "    return token.text in colors\n",
        "\n",
        "# Set extension on the Token with getter\n",
        "Token.set_extension(name=\"is_color\", getter=get_is_color, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue\")\n",
        "print (doc[3]._.is_color, doc[3].text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True blue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ0NBFGUDpfY",
        "colab_type": "text"
      },
      "source": [
        "Span extensions should almost always use a getter\n",
        "\n",
        "Why? Let us work through an example.\n",
        "\n",
        "Say we have 2 docs and we want to check if any of the first 3 words are colors in the set `['red', 'green', 'blue']` or not\n",
        "\n",
        "**Without getter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejwy2zw6ArOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43ac105a-3ed0-4110-84c9-bbfe7190b7c2"
      },
      "source": [
        "# set the colors\n",
        "colors = [\"red\", \"green\", \"blue\"]\n",
        "# init the 2 docs\n",
        "doc1, doc2 = nlp(\"hi little mini\"), nlp(\"mini likes green\")\n",
        "# init the spans\n",
        "span1, span2 = doc1[:3], doc2[:3]\n",
        "# set the attributes of the 2 spans\n",
        "Span.set_extension(name='is_color', default=False, force=True)\n",
        "\n",
        "# for each token check if the token is in colors and set the attribute of the span\n",
        "for token in span1:\n",
        "    if token.text in colors:\n",
        "        span1._.is_color=True\n",
        "\n",
        "for token in span2:\n",
        "    if token.text in colors:\n",
        "        span2._.is_color=True\n",
        "\n",
        "print (span1._.is_color, span2._.is_color)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enOp0mjlLK_U",
        "colab_type": "text"
      },
      "source": [
        "**Cleaner way:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t2xS1NGEX7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78685539-a318-4c43-d988-030189331486"
      },
      "source": [
        "# init the 2 docs\n",
        "doc1, doc2 = nlp(\"hi little mini\"), nlp(\"mini likes green\")\n",
        "# init the spans\n",
        "span1, span2 = doc1[:3], doc2[:3]\n",
        "\n",
        "def get_has_color(span):\n",
        "    colors = [\"red\", \"green\", \"blue\"]\n",
        "    return any(token.text in colors for token in span)\n",
        "\n",
        "# Set extension on the Span with getter\n",
        "Span.set_extension(name='is_color', getter=get_has_color, force=True)\n",
        "\n",
        "print (span1._.is_color, span2._.is_color)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5wXiCjSLKIE",
        "colab_type": "text"
      },
      "source": [
        "In this example, the get_has_color function takes the span and returns whether the text of any of the tokens is in the list of colors.\n",
        "\n",
        "**Method extensions**\n",
        "\n",
        "Method extensions make the extension attribute a callable method.\n",
        "\n",
        "You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting.\n",
        "\n",
        "In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, token_text.\n",
        "\n",
        "Here, the custom ._.has_token method returns True for the word \"blue\" and False for the word \"cloud\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TTffJd3K_7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "164fc300-517c-4f81-f024-fe3e6f381959"
      },
      "source": [
        "def has_token(doc, token_text):\n",
        "    in_doc = token_text in [token.text for token in doc] \n",
        "    return in_doc\n",
        "    \n",
        "Doc.set_extension(name=\"has_token\", method=has_token, force=True)\n",
        "\n",
        "doc = nlp(\"The sky is blue.\")\n",
        "print(doc._.has_token(\"blue\"), \"- blue\")\n",
        "print(doc._.has_token(\"cloud\"), \"- cloud\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True - blue\n",
            "False - cloud\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h0uXCiBL_TT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}