{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Research Series.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuGtRHkzdYvCpGC3NhzVGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Natural-Language-Processing/blob/master/BERT_Research_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPgk0koX4F7N"
      },
      "source": [
        "## Understanding BERT: Research Series\r\n",
        "\r\n",
        "> Based on the lecture series by \r\n",
        "ChrisMcCormickAI: https://www.youtube.com/playlist?list=PLam9sigHPGwOBuH4_4fr-XvDbe5uneaf6\r\n",
        "\r\n",
        "### Wordpiece model and Tokenizer\r\n",
        "\r\n",
        "Lets start with the input to a BERT model. An important part of that is basically to convert every word into an embedding\r\n",
        "\r\n",
        "BERT has a vocab of 30k unique tokens and each such token has 768 features (embedding dimension)\r\n",
        "\r\n",
        "So using these embeddings we go from:\r\n",
        "\r\n",
        "`seq of strings -> seq of embeddings`\r\n",
        "\r\n",
        "The distance bw the word embeddings indicates the similarity\r\n",
        "\r\n",
        "One thing to understand about word embeddings is that the actual feature values do not really matter as long as the relative distances remain the same\r\n",
        "\r\n",
        "#### Word embeddings in BERT:\r\n",
        "\r\n",
        "- BERT is pre-trained and has a __fixed__ vocab. How does it handle OOV words? \r\n",
        "    \r\n",
        "For example, the word `embedding` in OOV\r\n",
        "But the words em+bed+ding are in vocab and we have the features\r\n",
        "\r\n",
        "So BERT basically uses the features if the sub words. It does not average them up like in FastText, but simply considers them as new tokens. So if in the original sentence there were 10 tokens, now we will have 12\r\n",
        "\r\n",
        "- What happens for an unusual word which has no meanigful subword? \r\n",
        "\r\n",
        "BERT has a subword for every character in the alphabet\r\n",
        "\r\n",
        "![](https://i.imgur.com/tVS9MJ3.png)\r\n",
        "\r\n",
        "Probably nothing meaningful really comes out of this. \r\n",
        "\r\n",
        "But for other instances it might be helpful, like for different forms of the same word:\r\n",
        "\r\n",
        "![](https://i.imgur.com/anISgYu.png)\r\n",
        "\r\n",
        "Even though BERT knows nothing about `flabbergasts` it can get a good idea by considering the subwords\r\n",
        "\r\n",
        "#### Subword encoding in BERT\r\n",
        "\r\n",
        "![](https://i.imgur.com/eMCpAqw.png)\r\n",
        "\r\n",
        "Notice how each subword except the first one is preceeded with \"##\"\r\n",
        "\r\n",
        "This is just not a formatting thing, but the strings are actually defined this way internally in BERT\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFP89nmWGYK2"
      },
      "source": [
        "### Explore BERT Vocab\r\n",
        "\r\n",
        "1. Install huggingface implementation of BERT:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmg3q7Of3zkR",
        "outputId": "7abed4d8-6450-4c04-e1af-502fa859190d"
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.19.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.8)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.19.46)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.46->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.46->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ponvD3KfGhaA",
        "outputId": "1e4938f7-1425-4f81-f256-961f590f0f94"
      },
      "source": [
        "import torch\r\n",
        "from pytorch_pretrained_bert import BertTokenizer\r\n",
        "\r\n",
        "# Load pre-trained model tokenizer (vocabulary)\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 672520.67B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IrLQgG2c-8g"
      },
      "source": [
        "#### Vocab Dump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajmP8UwHdc8g",
        "outputId": "446cc935-6fa4-489b-a2de-831ebdb197f4"
      },
      "source": [
        "len(tokenizer.vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVmHuOFIdwQB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zntS517JcuCd"
      },
      "source": [
        "with open(\"vocabulary_BERT_BASE_UNCASED.txt\", \"w\") as f:\r\n",
        "    # for each token\r\n",
        "    for token in tokenizer.vocab.keys():\r\n",
        "        f.write(token + '\\n')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yd7G1v_dsAX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}