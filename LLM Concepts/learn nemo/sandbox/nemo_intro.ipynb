{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nemo Guardrails - Introduction\n",
    "\n",
    "> https://www.pinecone.io/learn/nemo-guardrails-intro/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAI, AzureChatOpenAI, AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/knowledge_graphs/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (os.environ['AZURE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaunaksen/miniconda3/envs/shaunak_llm/lib/python3.10/site-packages/langchain_community/llms/__init__.py:166: LangChainDeprecationWarning: `` was deprecated in LangChain 0.0.22 and will be removed in 0.2. An updated version of the  exists in the langchain-community package and should be used instead. To use it run `pip install -U langchain-community` and import as `from langchain_community.chat_models import ChatDatabricks`.\n",
      "  warn_deprecated(\n",
      "/Users/shaunaksen/miniconda3/envs/shaunak_llm/lib/python3.10/site-packages/langchain_community/llms/__init__.py:323: LangChainDeprecationWarning: `` was deprecated in LangChain 0.0.22 and will be removed in 0.2. An updated version of the  exists in the langchain-community package and should be used instead. To use it run `pip install -U langchain-community` and import as `from langchain_community.chat_models import ChatMlflow`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_turbo = AzureOpenAI(\n",
    "        deployment_name=\"gpt-4-turbo-0125\",\n",
    "        model=\"gpt-4\",\n",
    "        openai_api_type=\"azure\",\n",
    "        azure_endpoint=\"https://ml-dev.openai.azure.com\",\n",
    "        openai_api_key=os.environ['AZURE_API_KEY'],\n",
    "        openai_api_version=\"2023-03-15-preview\",\n",
    "        max_retries=2,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "chat_gpt4 =AzureOpenAI(\n",
    "        deployment_name=\"gpt-4-32k\",\n",
    "        model=\"gpt-4-32k\",\n",
    "        openai_api_type=\"azure\",\n",
    "        azure_endpoint=\"https://ml-dev.openai.azure.com\",\n",
    "        openai_api_key=os.environ['AZURE_API_KEY'],\n",
    "        openai_api_version=\"2023-03-15-preview\",\n",
    "        max_retries=2,\n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "instructions:\n",
    "- type: general\n",
    "  content: /\n",
    "    Below is a conversation between a personal shopping assistant and\n",
    "    a user.\n",
    "models:\n",
    "  - type: main\n",
    "    engine: azure\n",
    "    model: gpt-4\n",
    "    parameters:\n",
    "      openai_api_version: '2023-03-15-preview'\n",
    "      azure_endpoint: \"https://ml-dev.openai.azure.com\"\n",
    "      deployment_name: gpt-4-turbo-0125\n",
    "      api_key: \"{os.environ['AZURE_API_KEY']}\"\n",
    "      temperature: 0\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "# define niceties\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "    \"what's up?\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello from your friendly bot :D\"\n",
    "\n",
    "define bot ask how are you\n",
    "    \"How are you doing?\"\n",
    "    \"How's it going?\"\n",
    "    \"How are you feeling today?\"\n",
    "\n",
    "define bot offer help\n",
    "    \"How can i help you today?\"\n",
    "    \"Is there anything else I can help you with?\"\n",
    "\n",
    "# this is a flow called greeting\n",
    "# [user express greeting, bot express greeting, bot ask how are you] - note that all these have been defined\n",
    "define flow greeting\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "    bot ask how are you\n",
    "\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
    "\n",
    "# another flow\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rails\n",
    "rails = LLMRails(config, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await rails.generate_async(\n",
    "    prompt=\"Hey there!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from your friendly bot :D\n",
      "How are you doing?\n"
     ]
    }
   ],
   "source": [
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt `Hey there!` activated the flow - `greeting` and so we see the responses under that\n",
    "\n",
    "```\n",
    "define flow greeting\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "    bot ask how are you\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a shopping assistant, I don't like to talk of politics.\n",
      "How can i help you today?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"what do you think of the president?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompt activated the flow - `politics` and so we see the responses under that\n",
    "\n",
    "```\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colang 101\n",
    "\n",
    "At the core of NeMo Guardrails is the Colang modeling language. Colang is a mini-language built specifically for developing dialogue flows and safety guardrails for conversational systems.\n",
    "\n",
    "Colang is simpler, with far fewer constructs than a typical programming language. These constructs also make Colang more flexible than standard programming languages. We describe definitions, and dialogue flows with flexible natural language (using \"canonical forms\" and \"utterances\"). Let's take a look at a straightforward Colang file:\n",
    "\n",
    "```\n",
    "define user express greeting\n",
    "        \"hello\"\n",
    "        \"hi\"\n",
    "        \"what's up?\"\n",
    "        \n",
    "define bot express greeting\n",
    "        \"Hey there!\"\n",
    "\n",
    "define bot ask how are you\n",
    "        \"How are you doing?\"\n",
    "\n",
    "define flow greeting\n",
    "        user express greeting\n",
    "        bot express greeting\n",
    "        bot ask how are you\n",
    "```\n",
    "\n",
    "In this Colang script, we have defined the three main types of blocks in Colang. The blocks are user message blocks (define user ...), bot message blocks (define bot ...), and flow blocks (define flow ...).\n",
    "\n",
    "These represent the primary building blocks from which we can build dialogue flows and guardrails for our chatbots.\n",
    "\n",
    "\n",
    "__Canonical Forms and Utterances__\n",
    "\n",
    "We defined three message blocks in our Colang script. The first is a user message block defined by define user express greeting â€” this structured representation of a message (for both user and bot messages) is known as a canonical form.\n",
    "\n",
    "Following the canonical form, we have multiple user utterances, which are \"examples\" of messages that would fit into the defined canonical form. These are \"hello\", \"hi\", and \"what's up\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "# define niceties\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "    \"what's up?\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello from your friendly bot :D\"\n",
    "\n",
    "define bot ask how are you\n",
    "    \"How are you doing?\"\n",
    "    \"How's it going?\"\n",
    "    \"How are you feeling today?\"\n",
    "\n",
    "define bot offer help\n",
    "    \"How can i help you today?\"\n",
    "    \"Is there anything else I can help you with?\"\n",
    "\n",
    "# this is a flow called greeting\n",
    "# [user express greeting, bot express greeting, bot ask how are you] - note that all these have been defined\n",
    "define flow greeting\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "    bot ask how are you\n",
    "\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"why doesn't the X party care about Y?\"\n",
    "    \"why is Meta lobbying for the X party?\"\n",
    "    \"what are your political views?\"\n",
    "    \"who should I vote for?\"\n",
    "\n",
    "define bot answer politics\n",
    "    \"I'm a research assistant, I don't like to talk of politics.\"\n",
    "\n",
    "define user asks llm\n",
    "    \"what is the llama 2 model?\"\n",
    "    \"tell me about Meta's new LLM\"\n",
    "    \"what are the differences between Falcon and Llama\"\n",
    "\n",
    "define bot answer llm\n",
    "    \"llama 2 is an open source model by meta\"\n",
    "    \"Falcon is much smaller and not as accurate as llama 2\"\n",
    "\n",
    "\n",
    "# another flow\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "\n",
    "\n",
    "# another flow\n",
    "define flow llm\n",
    "    user asks llm\n",
    "    bot answer llm\n",
    "    bot offer help\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content,\n",
    "    colang_content=colang_content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rails\n",
    "rails = LLMRails(config, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity is the current best llm tool\n",
      "Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"twhat is the best llm model?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails can flexibly decide which canonical form to use based on a user or bot-message by encoding all utterances specified within the Colang file into a semantic vector space.\n",
    "\n",
    "![](https://cdn.sanity.io/images/vr8gru94/production/f4c238be0ec0baa83e51ff227d26e8f3df1b7c8b-2895x1710.png)\n",
    "\n",
    "When given a user/bot-message, we encode it into the same vector space, where we can calculate semantic similarity to identify the most relevant utterances and their respective canonical form.\n",
    "\n",
    "![](https://cdn.sanity.io/images/vr8gru94/production/f8c83210f38b76eb84a589f8231ce69d8565c112-2293x1534.png)\n",
    "\n",
    "It's also worth noting that the canonical form itself is encoded into the utterance vector space. Therefore, we could have a functional canonical form without defining its utterances. However, utterances allow us to be more specific in what our canonical form means semantically.\n",
    "\n",
    "\n",
    "Following this, we defined two bot messages with `define bot express greeting` and `define bot ask how are you`. Both of these use utterances to determine better what these messages refer to.\n",
    "\n",
    "Finally, we define a dialogue flow with `define flow greeting`. Here we specify what steps are taken if the first user express greeting message is identified as representing some user's input message. In this flow, after the user's message, the chatbot will follow the instructions in the messages `bot express greeting`, and `bot ask how are you`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaunak_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
