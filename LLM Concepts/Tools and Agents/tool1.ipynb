{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Tools - LangChain handbook\n",
    "\n",
    "> https://www.pinecone.io/learn/series/langchain/langchain-tools/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Calculator Tool\n",
    "\n",
    "We will start with a simple custom tool. The tool is a simple calculator that calculates a circle’s circumference based on the circle’s radius.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import pi, sqrt, cos, sin\n",
    "from typing import Union, Optional, Type\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models and embeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI, HuggingFaceHub\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# prompts and utils\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# loaders and splitters\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "\n",
    "\n",
    "# memory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# chains\n",
    "from langchain.chains import LLMChain, ConversationChain, RetrievalQA\n",
    "\n",
    "# vectorstores\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "# agents and tools\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.agents import initialize_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/knowledge_graphs/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_API_KEY = os.getenv('AZURE_API_KEY')\n",
    "AZURE_API_BASE = os.getenv('AZURE_API_BASE')\n",
    "AZURE_API_VERSION = os.getenv('AZURE_API_VERSION')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models() -> list:\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment=\"text-embedding-ada-002\",\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_type='azure',\n",
    "        azure_endpoint=AZURE_API_BASE,\n",
    "        openai_api_key=AZURE_API_KEY,\n",
    "        openai_api_version=AZURE_API_VERSION,\n",
    "        chunk_size=1, max_retries=1e0\n",
    "    )\n",
    "\n",
    "    llm_chat_gpt_4 = AzureChatOpenAI(deployment_name='gpt-4-32k',\n",
    "                          model='gpt-4-32k',\n",
    "                          openai_api_type='azure',\n",
    "                          azure_endpoint=AZURE_API_BASE,\n",
    "                          openai_api_key=AZURE_API_KEY,\n",
    "                          openai_api_version=AZURE_API_VERSION,\n",
    "                          max_retries=2,\n",
    "                          temperature=0,\n",
    "                          streaming=True\n",
    "                          )\n",
    "\n",
    "    llm_gpt_4 = AzureOpenAI(deployment_name='gpt-4-32k',\n",
    "                            model='gpt-4-32k',\n",
    "                            openai_api_type='azure',\n",
    "                            azure_endpoint=AZURE_API_BASE,\n",
    "                            openai_api_key=AZURE_API_KEY,\n",
    "                            openai_api_version=AZURE_API_VERSION,\n",
    "                            max_retries=2,\n",
    "                            temperature=0,\n",
    "                            streaming=True\n",
    "                            )\n",
    "\n",
    "    llm_text_davinci = AzureOpenAI(deployment_name='text-davinci-003',\n",
    "                          model='text-davinci-003',\n",
    "                          openai_api_type='azure',\n",
    "                          azure_endpoint=AZURE_API_BASE,\n",
    "                          openai_api_key=AZURE_API_KEY,\n",
    "                          openai_api_version=AZURE_API_VERSION,\n",
    "                          max_retries=2,\n",
    "                          temperature=0)\n",
    "\n",
    "    hub_llm = HuggingFaceHub(\n",
    "        repo_id=\"google/flan-t5-large\",\n",
    "        model_kwargs={\"temperature\": 0}\n",
    "    )\n",
    "\n",
    "    return [embeddings, llm_chat_gpt_4, llm_gpt_4, llm_text_davinci, hub_llm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, llm_chat_gpt_4, llm_gpt_4, llm_text_davinci, hub_llm = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tell me an astronaut joke\"\n",
    "print(llm_text_davinci(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"Circumference calculator\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi\n",
    "    \n",
    "    def _arun(self, radius: Union[int, float]):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialized our custom CircumferenceTool class using the BaseTool object from LangChain. We can think of the BaseTool as the required template for a LangChain tool.\n",
    "\n",
    "We have two attributes that LangChain requires to recognize an object as a valid tool. Those are the name and description parameters.\n",
    "\n",
    "The description is a natural language description of the tool the LLM uses to decide whether it needs to use it. Tool descriptions should be very explicit on what they do, when to use them, and when not to use them.\n",
    "\n",
    "> In our description, we did not define when not to use the tool. That is because the LLM seemed capable of identifying when this tool is needed. Adding “when not to use it” to the description can help if a tool is overused.\n",
    "\n",
    "Following this, we have two methods, _run and _arun. When a tool is used, the _run method is called by default. The _arun method is called when a tool is to be used asynchronously. We do not cover async tools in this chapter, so, for now, we initialize it with a NotImplementedError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the LLM with a temperature of 0. A low temperature is useful when using tools as it decreases the amount of “randomness” or “creativity” in the generated text of the LLMs, which is ideal for encouraging it to follow strict instructions — as required for tool usage.\n",
    "\n",
    "In the conversation_memory object, we set k=5 to “remember” the previous five human-AI interactions.\n",
    "\n",
    "Now we initialize the agent itself. It requires the llm and conversational_memory to be already initialized. It also requires a list of tools to be used. We have one tool, but we still place it into a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [CircumferenceTool()]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm_chat_gpt_4,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent type of chat-conversation-react-description tells us a few things about this agent, those are:\n",
    "\n",
    "- chat means the LLM being used is a chat model. Both gpt-4 and gpt-3.5-turbo are chat models as they consume conversation history and produce conversational responses. A model like text-davinci-003 is not a chat model as it is not designed to be used this way.\n",
    "\n",
    "- conversational means we will be including conversation_memory.\n",
    "\n",
    "- react refers to the ReAct framework, which enables multi-step reasoning and tool usage by giving the model the ability to “converse with itself”.\n",
    "\n",
    "- description tells us that the LLM/agent will decide which tool to use based on their descriptions — which we created in the earlier tool definition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"can you calculate the circumference of a circle that has a radius of 7.81mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.81 * 2 * pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This works here (maybe because we are using gpt-4) but in the article this did not work, so for the next step we will assume that the ans returned is not perfect\n",
    "\n",
    "The Final Answer action is what the agent uses when it has decided it has completed its reasoning and action steps and has all the information it needs to answer the user’s query. That means the agent decided not to use the circumference calculator tool.\n",
    "\n",
    "LLMs are generally bad at math, but that doesn’t stop them from trying to do math. The problem is due to the LLM’s overconfidence in its mathematical ability. To fix this, we must tell the model that it cannot do math. First, let’s see the current prompt being used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(agent.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (agent.agent.llm_chain.prompt.messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (agent.agent.llm_chain.prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a single sentence that tells the model that it is “terrible at math” and should never attempt to do it.\n",
    "\n",
    "```\n",
    "Unfortunately, the Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to its trusty tools and absolutely does NOT try to answer math questions by itself\n",
    "```\n",
    "\n",
    "With this added to the original prompt text, we create a new prompt using agent.agent.create_prompt — this will create the correct prompt structure for our agent, including tool descriptions. Then, we update agent.agent.llm_chain.prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "\n",
    "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "\n",
    "Unfortunately, Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to it's trusty tools and absolutely does NOT try to answer math questions by itself\n",
    "\n",
    "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"can you calculate the circumference of a circle that has a radius of 7.81mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools With Multiple Parameters\n",
    "\n",
    "In the circumference calculator, we could only input a single value — the radius — more often than not, we will need multiple parameters.\n",
    "\n",
    "\n",
    "To demonstrate how to do this, we will build a Hypotenuse calculator. The tool will help us calculate the hypotenuse of a triangle given a combination of triangle side lengths and/or angles.\n",
    "\n",
    "\n",
    "We want multiple inputs here because we calculate the triangle hypotenuse with different values (the sides and angle). Additionally, we don’t need all values. We can calculate the hypotenuse with any combination of two or more parameters.\n",
    "\n",
    "We define our new tool like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = (\n",
    "    \"use this tool when you need to calculate the length of a hypotenuse\"\n",
    "    \"given one or two sides of a triangle and/or an angle (in degrees). \"\n",
    "    \"To use the tool, you must provide at least two of the following parameters \"\n",
    "    \"['adjacent_side', 'opposite_side', 'angle'].\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythagorasTool(BaseTool):\n",
    "    name = \"Hypotenuse calculator\"\n",
    "    description = desc\n",
    "\n",
    "    def _run(\n",
    "            self, \n",
    "            adjacent_side: Optional[Union[int, float]]=None,\n",
    "            opposite_side: Optional[Union[int, float]]=None,\n",
    "            angle: Optional[Union[int, float]]=None,\n",
    "    ):\n",
    "        print (adjacent_side, opposite_side, angle)\n",
    "        # check for the values we have been given\n",
    "        if adjacent_side and opposite_side:\n",
    "            return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n",
    "        elif adjacent_side and angle:\n",
    "            return adjacent_side / cos(float(angle))\n",
    "        elif opposite_side and angle:\n",
    "            return opposite_side / sin(float(angle))\n",
    "        else:\n",
    "            return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n",
    "        \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PythagorasTool._run(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [PythagorasTool()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tool description, we describe the tool functionality in natural language and specify that to “use the tool, you must provide at least two of the following parameters [‘adjacent_side’, ‘opposite_side’, ‘angle’]\". This instruction is all we need for gpt-3.5-turbo to understand the required input format for the function.\n",
    "\n",
    "As before, we must update the agent’s prompt. We don’t need to modify the system message as we did before, but we do need to update the available tools described in the prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=sys_msg,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "agent.agent.llm_chain.prompt = new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must also update the agent.tools attribute with our new tools\n",
    "agent.tools = tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"If I have a triangle with adjacent side of length 51cm and opposite side of length 34cm, what is the length of the hypotenuse?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"If I have a triangle with the opposite side of length 51cm and an angle of 20 deg, what is the length of the hypotenuse?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does not seem to work\n",
    "\n",
    "\n",
    "Let us go through langchain offcial guide to learn more and see if we can correct this down the line\n",
    "\n",
    "## Defining Custom Tools\n",
    "\n",
    "> https://python.langchain.com/docs/modules/agents/tools/custom_tools\n",
    "\n",
    "---\n",
    "\n",
    "When constructing your own agent, you will need to provide it with a list of Tools that it can use. Besides the actual function that is called, the Tool consists of several components:\n",
    "\n",
    "- name (str), is required and must be unique within a set of tools provided to an agent\n",
    "\n",
    "- description (str), is optional but recommended, as it is used by an agent to determine tool use\n",
    "\n",
    "- args_schema (Pydantic BaseModel), is optional but recommended, can be used to provide more information (e.g., few-shot examples) or validation for expected parameters.\n",
    "\n",
    "\n",
    "There are multiple ways to define a tool. In this guide, we will walk through how to do for two functions:\n",
    "\n",
    "1. A made up search function that always returns the string “LangChain”\n",
    "\n",
    "2. A multiplier function that will multiply two numbers by eachother\n",
    "\n",
    "\n",
    "The biggest difference here is that the first function only requires one input, while the second one requires multiple. Many agents only work with functions that require single inputs, so it’s important to know how to work with those. For the most part, defining these custom tools is the same, but there are some differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `@tool` decorator is the simplest way to define a custom tool. The decorator uses the function name as the tool name by default, but this can be overridden by passing a string as the first argument. Additionally, the decorator will use the function’s docstring as the tool’s description - so a docstring MUST be provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"search\")\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Look up things online.\n",
    "    \"\"\"\n",
    "    return \"Mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.name)\n",
    "print(search.description)\n",
    "print(search.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also customize the tool name and JSON args by passing them into the tool decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\", default=\"what is meaning of life?\")\n",
    "\n",
    "@tool(\"search-tool\", args_schema=SearchInput, return_direct=True)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Look up things online.\n",
    "    \"\"\"\n",
    "    return \"Mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.name)\n",
    "print(search.description)\n",
    "print(search.args)\n",
    "print(search.return_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclass BaseTool\n",
    "\n",
    "You can also explicitly define a custom tool by subclassing the BaseTool class. This provides maximal control over the tool definition, but is a bit more work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lets define the schema classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"should be a search query\")\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: Union[int, str] = Field(description=\"first number\")\n",
    "    b: Union[int, str] = Field(description=\"second number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the Custom search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSearchTool(BaseTool):\n",
    "    name = \"custom_search\"\n",
    "    description = \"useful for when you need to answer questions about current events\"\n",
    "    args_schema: Type[BaseModel] = SearchInput\n",
    "\n",
    "    def _run(\n",
    "            self, query: str, run_manager: Optional[CallbackManagerForToolRun]=None\n",
    "    ):\n",
    "        \"\"\"Use the tool.\"\"\"\n",
    "        return \"Mini\"\n",
    "    \n",
    "    async def _arun(\n",
    "            self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun]=None\n",
    "    ):\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"custom_search does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = CustomSearchTool()\n",
    "print(search.name)\n",
    "print(search.description)\n",
    "print(search.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCalculatorTool(BaseTool):\n",
    "    name = \"Calculator\"\n",
    "    description = \"useful for when you need to answer questions about math\"\n",
    "    args_schema: Type[BaseModel] = CalculatorInput\n",
    "    return_direct: bool=True\n",
    "\n",
    "    def _run(\n",
    "            self, a: Union[int, str], b: Union[int, str], run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Use the tool\n",
    "        \"\"\"\n",
    "        a = int(a)\n",
    "        b= int(b)\n",
    "        return a*b\n",
    "    \n",
    "    def _arun(\n",
    "            self, a: int, b: int, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use the tool asynchronously.\"\"\"\n",
    "        raise NotImplementedError(\"Calculator does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply = CustomCalculatorTool()\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)\n",
    "print(multiply.return_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [CustomCalculatorTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm_chat_gpt_4, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is 7 times 7?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [CustomCalculatorTool()]\n",
    "\n",
    "# initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm_chat_gpt_4,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatorInput(BaseModel):\n",
    "    a: list = Field(description=\"first number\")\n",
    "    b: Union[int, str] = Field(description=\"second number\")\n",
    "\n",
    "\n",
    "def multiply(a: Union[int, str], b: Union[int, str]) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return int(a) * int(b)\n",
    "\n",
    "\n",
    "calculator = StructuredTool.from_function(\n",
    "    func=multiply,\n",
    "    name=\"Calculator\",\n",
    "    description=\"multiply numbers\",\n",
    "    args_schema=CalculatorInput,\n",
    "    return_direct=True,\n",
    "    # coroutine= ... <- you can specify an async method if desired as well\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [calculator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm_chat_gpt_4, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"what is 7 times 7?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaunak_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
