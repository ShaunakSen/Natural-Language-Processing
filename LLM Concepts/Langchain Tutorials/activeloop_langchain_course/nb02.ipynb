{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActiveLoop Course - LangChain & Vector Databases in Production\n",
    "\n",
    "> https://learn.activeloop.ai/enrollments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/mini_projects/chatgpt_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrap(text, width=100):\n",
    " \"\"\"\n",
    " Prints text wrapped to a set width.\n",
    "\n",
    " Args:\n",
    "   text: The text to wrap.\n",
    "   width: The maximum width of each line.\n",
    "\n",
    " Returns:\n",
    "   None\n",
    " \"\"\"\n",
    " words = text.split()\n",
    " current_line = \"\"\n",
    " for word in words:\n",
    "   if len(current_line) + len(word) + 1 > width:\n",
    "     print(current_line)\n",
    "     current_line = word\n",
    "   else:\n",
    "     current_line += \" \" + word\n",
    " if current_line:\n",
    "   print(current_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# models and embeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI, HuggingFaceHub\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "# prompts and utils\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# chains\n",
    "from langchain.chains import LLMChain, ConversationChain, RetrievalQA\n",
    "\n",
    "# vectorstores\n",
    "from langchain.vectorstores import DeepLake, FAISS\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "# agents and tools\n",
    "from langchain.agents import initialize_agent, Tool, load_tools\n",
    "from langchain.agents import AgentType\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from creds import (AZURE_API_BASE, AZURE_API_KEY, AZURE_API_VERSION, \n",
    "                   ACTIVELOOP_TOKEN, GOOGLE_API_KEY, GOOGLE_CSE_ID,\n",
    "                   HUGGINGFACEHUB_API_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models() -> list:\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        deployment=\"text-embedding-ada-002\",\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        openai_api_type='azure',\n",
    "        azure_endpoint=AZURE_API_BASE,\n",
    "        openai_api_key=AZURE_API_KEY,\n",
    "        openai_api_version=AZURE_API_VERSION,\n",
    "        chunk_size=1, max_retries=1e0 \n",
    "    )\n",
    "\n",
    "    llm_chat_gpt_4 = AzureChatOpenAI(deployment_name='gpt-4-32k',\n",
    "                          model='gpt-4-32k',\n",
    "                          openai_api_type='azure',\n",
    "                          azure_endpoint=AZURE_API_BASE,\n",
    "                          openai_api_key=AZURE_API_KEY,\n",
    "                          openai_api_version=AZURE_API_VERSION,\n",
    "                          max_retries=2,\n",
    "                          temperature=0,\n",
    "                          streaming=True\n",
    "                          )\n",
    "\n",
    "    llm_gpt_4 = AzureOpenAI(deployment_name='gpt-4-32k',\n",
    "                            model='gpt-4-32k',\n",
    "                            openai_api_type='azure',\n",
    "                            azure_endpoint=AZURE_API_BASE,\n",
    "                            openai_api_key=AZURE_API_KEY,\n",
    "                            openai_api_version=AZURE_API_VERSION,\n",
    "                            max_retries=2,\n",
    "                            temperature=0,\n",
    "                            streaming=True\n",
    "                            )\n",
    "    \n",
    "    llm_text_davinci = AzureOpenAI(deployment_name='text-davinci-003',\n",
    "                          model='text-davinci-003',\n",
    "                          openai_api_type='azure',\n",
    "                          azure_endpoint=AZURE_API_BASE,\n",
    "                          openai_api_key=AZURE_API_KEY,\n",
    "                          openai_api_version=AZURE_API_VERSION,\n",
    "                          max_retries=2,\n",
    "                          temperature=0.5)\n",
    "    \n",
    "    hub_llm = HuggingFaceHub(\n",
    "        repo_id=\"google/flan-t5-large\",\n",
    "        model_kwargs={\"temperature\": 0}\n",
    "    )\n",
    "    \n",
    "    return [embeddings, llm_chat_gpt_4, llm_gpt_4, llm_text_davinci, hub_llm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaunaksen/miniconda3/envs/aiops_venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "embeddings, llm_chat_gpt_4, llm_gpt_4, llm_text_davinci, hub_llm = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Start with a warm-up of 5-10 minutes of light jogging and dynamic stretching.\n",
      "\n",
      "2. Perform a HIIT workout - alternating between sprinting for 30 seconds and jogging for 90 seconds for a total of 20 minutes.\n",
      "\n",
      "3. Follow up with a 7-10 minute jog at a moderate pace.\n",
      "\n",
      "4. Finish with 5-10 minutes of stretching and cool down.\n",
      "\n",
      "5. Repeat this workout 3-4 times a week.\n",
      "\n",
      "6. On the other days, go for a 30-60 minute walk or jog at a moderate pace.\n",
      "\n",
      "7. Mix it up by adding in outdoor activities such as biking, swimming, or rowing.\n"
     ]
    }
   ],
   "source": [
    "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "print(llm_text_davinci(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot learning\n",
    "\n",
    "Few-shot learning is a remarkable ability that allows LLMs to learn and generalize from limited examples. Prompts serve as the input to these models and play a crucial role in achieving this feature. With LangChain, examples can be hard-coded, but dynamically selecting them often proves more powerful, enabling LLMs to adapt and tackle tasks with minimal training data swiftly.\n",
    "\n",
    "This approach involves using the `FewShotPromptTemplate` class, which takes in a `PromptTemplate` and a list of a few shot examples. The class formats the prompt template with a few shot examples, which helps the language model generate a better response. We can streamline this process by utilizing LangChain's FewShotPromptTemplate to structure the approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are excerpts from conversations with an AI\n",
      "assistant. The assistant is known for its humor and wit, providing\n",
      "entertaining and amusing responses to users' questions. Here are some\n",
      "examples:\n",
      "\n",
      "\n",
      "\n",
      "User: What's the weather like?\n",
      "AI: It's raining cats and dogs, better bring an umbrella!\n",
      "\n",
      "\n",
      "\n",
      "User: How old are you?\n",
      "AI: Age is just a number, but I'm timeless.\n",
      "\n",
      "\n",
      "\n",
      "User: what is your favorite dish?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt_template.format(query=\"what is your favorite dish?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following are excerpts from conversations with an AI\n",
      "assistant. The assistant is known for its humor and wit, providing\n",
      "entertaining and amusing responses to users' questions. Here are some\n",
      "examples:\n",
      "\n",
      "\n",
      "\n",
      "User: What's the weather like?\n",
      "AI: It's raining cats and dogs, better bring an umbrella!\n",
      "\n",
      "\n",
      "\n",
      "User: How old are you?\n",
      "AI: Age is just a number, but I'm timeless.\n",
      "\n",
      "\n",
      "\n",
      "User: what is your favorite dish?\n",
      "AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' My favorite dish is anything that comes with a side of laughter!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = LLMChain(prompt=few_shot_prompt_template, llm=llm_text_davinci, verbose=True)\n",
    "chain.run(\"what is your favorite dish?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ask multiple questions, we can either iterate through all questions one at a time or place all questions into a single prompt for more advanced LLMs. Let's start with the first approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n",
      "giraffe\n",
      "nitrogen\n",
      "yellow\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "for gen_list in res.generations:\n",
    "    gen_text = gen_list[0].text\n",
    "    print( gen_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Translation\n",
    "\n",
    "It is one of the great attributes of Large Language models that enables them to perform multiple tasks just by changing the prompt. We use the same llmCopy variable as defined before. However, pass a different prompt that asks for translating the query from a source_languageCopy to the target_languageCopy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=['source_language', 'target_language', 'text'], template=translation_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_chain = LLMChain(llm=llm_text_davinci, prompt=translation_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mTranslate the following text from English to Hindi: Hello\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "नमस्ते\n"
     ]
    }
   ],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"Hindi\"\n",
    "text = \"Hello\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)\n",
    "print (translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiops_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
