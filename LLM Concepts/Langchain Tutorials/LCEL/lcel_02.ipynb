{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL \n",
    "\n",
    "> https://www.youtube.com/watch?v=8aUYzb1aYDU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnablePassthrough\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = \"\"\n",
    "os.environ['GROQ_API_KEY'] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I love programming in French is: \"J\\'aime programmer.\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 30, 'total_tokens': 46, 'completion_time': 0.024077454, 'prompt_time': 0.002665191, 'queue_time': 0.012173658, 'total_time': 0.026742645}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-d8c20fea-cf5a-4015-9bb1-944c297ac975-0', usage_metadata={'input_tokens': 30, 'output_tokens': 16, 'total_tokens': 46})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple chains using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['skill'] input_types={} partial_variables={} template='Hi! I am learning {skill}. Can you help point me to top 5 courses for it.'\n"
     ]
    }
   ],
   "source": [
    "template = \"Hi! I am learning {skill}. Can you help point me to top 5 courses for it.\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['skill'])\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9g/tslm33j92p1blxwl8vvjxn1r0000gn/T/ipykernel_18185/4035131671.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'd be happy to help you find some top-notch video editing courses. Here are five courses that I highly recommend:\n",
      "\n",
      "1. Adobe Premiere Pro Masterclass: Learn Video Editing in Premiere Pro - This course is great for beginners who want to learn Adobe Premiere Pro, one of the most popular video editing software. It covers all the basics and advanced techniques you need to know to create professional-looking videos.\n",
      "2. Video Editing with Adobe Premiere Pro for Beginners: Start Editing Video Like a Pro - This course is another excellent option for beginners who want to learn Adobe Premiere Pro. It covers the fundamentals of video editing, including cutting, transitions, and color correction.\n",
      "3. Final Cut Pro X: Video Editing Masterclass - If you're a Mac user, this course is an excellent choice for learning Final Cut Pro X, another popular video editing software. It covers everything from basic editing techniques to advanced color grading and audio mixing.\n",
      "4. Video Editing in DaVinci Resolve 17: The Complete Course - DaVinci Resolve is a powerful and free video editing software that's gaining popularity among professionals. This course covers all the basics and advanced techniques you need to know to edit videos like a pro.\n",
      "5. After Effects for Video Editors - This course is perfect for video editors who want to take their skills to the next level by learning motion graphics and visual effects. It covers the basics of Adobe After Effects, a popular software for creating animations and visual effects.\n",
      "\n",
      "These courses are all highly rated and taught by experienced instructors. They cover a range of topics and software, so you can choose the one that best fits your needs and goals. Good luck with your video editing journey!\n"
     ]
    }
   ],
   "source": [
    "print (llm_chain.run({\"skill\": \"Video editing\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm glad to hear that you're interested in machine learning engineering. Here are five top courses that can help you get started:\n",
      "\n",
      "1. Machine Learning Engineering on Coursera: This course is offered by the University of Washington and covers the entire machine learning engineering pipeline, from data preparation to model deployment. It includes hands-on projects and is a great way to get started with machine learning engineering.\n",
      "2. Machine Learning on edX: This course is offered by MIT and covers the fundamentals of machine learning, including supervised and unsupervised learning, linear models, and neural networks. It's a great course for those who want to build a strong foundation in machine learning before diving into engineering.\n",
      "3. Deep Learning Specialization on Coursera: This is a five-course specialization offered by Andrew Ng, a pioneer in the field of deep learning. The courses cover neural networks, deep learning, structuring machine learning projects, convolutional neural networks, and sequence models.\n",
      "4. Machine Learning Engineering on Udacity: This course is offered by Udacity and covers the entire machine learning engineering pipeline, including data preprocessing, model training, model evaluation, and model deployment. It includes hands-on projects and is a great way to learn machine learning engineering in a practical setting.\n",
      "5. Applied Machine Learning on DataCamp: This course covers the practical aspects of machine learning, including data preprocessing, feature engineering, model training, and model evaluation. It includes hands-on projects and is a great way to learn machine learning engineering in a hands-on setting.\n",
      "\n",
      "I hope these courses help you on your journey to becoming a machine learning engineer!\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print (chain.invoke({\"skill\": \"ML Engineering\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each elem in the LCEL should have an `invoke()` method\n",
    "\n",
    "\"invoke\" in dir(prompt), \"invoke\" in dir(llm), \"invoke\" in dir(StrOutputParser())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"invoke\" in dir(RunnablePassthrough())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough() | RunnablePassthrough() | RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mini'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnablePassthrough` is used to take an input and pass that to the next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_upper(input_str: str) -> str:\n",
    "    return input_str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda` is used to run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough() | RunnableLambda(string_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MINI'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"invoke\" in dir(string_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 'mini', 'y': 'mini'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel({\"x\": RunnablePassthrough(), \"y\": RunnablePassthrough()})\n",
    "chain.invoke(\"mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'details': {'name': 'mini', 'age': 28}, 'age': 28}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel({\"details\": RunnablePassthrough(), \"age\": lambda x: x['age']})\n",
    "chain.invoke({\"name\": \"mini\", \"age\": 28})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_name(x):\n",
    "    return x.get(\"food\", \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\n",
    "    \"food\": RunnablePassthrough() | RunnableLambda(fetch_name),\n",
    "    \"age\": lambda x: x['age']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': 'not found', 'age': 28}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"name\": \"mini\", \"age\": 28})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Basic RAG infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path = \"./data\", glob=\"./*.txt\", loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "new_docs = text_splitter.split_documents(\n",
    "    documents=docs\n",
    ")\n",
    "\n",
    "print (len(new_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhagarwal/.pyenv/versions/3.12.4/envs/poetry-env/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(\n",
    "    documents=new_docs, embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./chroma'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_chroma.vectorstores.Chroma"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_kwargs = {\n",
    "        \"k\": 4,\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the following question based ONLY on the retrieved context.\n",
    "If the answer does not occur in the context, reply: I do not know\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in proper markdown format:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/essay.txt'}, page_content=\"When I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted yet).\"),\n",
       " Document(metadata={'source': 'data/essay.txt'}, page_content=\"I was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around this time.)\\n\\nThe best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.\"),\n",
       " Document(metadata={'source': 'data/essay.txt'}, page_content='Working on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.'),\n",
       " Document(metadata={'source': 'data/essay.txt'}, page_content=\"The next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author's takeaway from YCombinator was that it was an engaging and effective way to learn about startups in a short amount of time due to the varied problems presented by the new batches of startups every 6 months. They also realized that YCombinator would eventually take up all of their attention, leading them to the decision that it couldn't be their life's work and they would have to leave eventually. Additionally, the author mentioned that YCombinator was renamed from Cambridge Seed to avoid a regional name and to differentiate from the staid color choices of other VCs. YCombinator also became a fund for a couple of years starting in 2009 due to its growing size, but later returned to being self-funded.\n",
      "Elapsed time: 0.8561 seconds\n"
     ]
    }
   ],
   "source": [
    "s_time = time.perf_counter()\n",
    "\n",
    "question = \"What was the author's takeaway from YCombinator?\"\n",
    "print(retrieval_chain.invoke(question))\n",
    "e_time = time.perf_counter()\n",
    "elapsed_time = e_time - s_time\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author's takeaway from YCombinator was that it was an engaging and effective way to learn about startups in a short amount of time due to the varied problems presented by the new batches of startups every 6 months. They also realized that YCombinator would eventually take up all of their attention, leading them to the decision that it couldn't be their life's work and they would have to leave eventually. Additionally, the author mentioned that YCombinator was renamed from Cambridge Seed to avoid a regional name and to differentiate from the staid color choices of other VCs. YCombinator also became a fund for a couple of years starting in 2009 due to its growing size, but later returned to being self-funded.\n",
      "Elapsed time: 0.8729 seconds\n"
     ]
    }
   ],
   "source": [
    "s_time = time.perf_counter()\n",
    "\n",
    "question = \"What was the author's takeaway from YCombinator?\"\n",
    "print(await retrieval_chain.ainvoke(question))\n",
    "e_time = time.perf_counter()\n",
    "elapsed_time = e_time - s_time\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `Itemgetter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": itemgetter(\"question\") | retriever,\n",
    "                      \"question\": itemgetter(\"question\"),\n",
    "                      \"language\": itemgetter(\"language\")\n",
    "                      })\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  +------------------------------------------+            \n",
      "                  | Parallel<context,question,language>Input |            \n",
      "                  +------------------------------------------+            \n",
      "                        *****           *           *****                 \n",
      "                    ****               *                 *****            \n",
      "                 ***                   *                      ****        \n",
      "       +--------+                      *                          ***     \n",
      "       | Lambda |                      *                            *     \n",
      "       +--------+                      *                            *     \n",
      "            *                          *                            *     \n",
      "            *                          *                            *     \n",
      "            *                          *                            *     \n",
      "+----------------------+          +--------+                   +--------+ \n",
      "| VectorStoreRetriever |          | Lambda |                 **| Lambda | \n",
      "+----------------------+          +--------+             ****  +--------+ \n",
      "                        *****          *            *****                 \n",
      "                             ****       *       ****                      \n",
      "                                 ***    *    ***                          \n",
      "                  +-------------------------------------------+           \n",
      "                  | Parallel<context,question,language>Output |           \n",
      "                  +-------------------------------------------+           \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                               +----------------+                         \n",
      "                               | PromptTemplate |                         \n",
      "                               +----------------+                         \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                                  +----------+                            \n",
      "                                  | ChatGroq |                            \n",
      "                                  +----------+                            \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                              +-----------------+                         \n",
      "                              | StrOutputParser |                         \n",
      "                              +-----------------+                         \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                                        *                                 \n",
      "                            +-----------------------+                     \n",
      "                            | StrOutputParserOutput |                     \n",
      "                            +-----------------------+                     \n"
     ]
    }
   ],
   "source": [
    "retrieval_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YCombinator-er autor ke nischoyeke bouddhho holo, YCombinator ekjon bhalo probaashho holo, kintu tini je shohoj holo na, tini ekdin birstho korte hobe. YCombinator tinike shohoj holo karon tini shohoj korechilo na, bhalo probaashho holo karon tini bhalo shikhte pari ekadin startups-er shohoj probaashho.\n",
      "\n",
      "(Note: This is a rough translation of the answer in Bengali language. The actual translation may vary based on the dialect and context.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### itemgetter only works with dictionaries , input has to be a dict\n",
    "\n",
    "response = retrieval_chain.invoke({'question': \"What was the author's takeaway from YCombinator?\",\n",
    "                        'language': \"bengali\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = 'Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! That's great to hear that you're learning Big Data. Here are the top 5 things that I would recommend you to learn:\n",
      "\n",
      "1. **Hadoop Ecosystem**: Hadoop is a popular open-source framework for storing and processing large datasets. You should learn the core components of the Hadoop ecosystem, such as HDFS (Hadoop Distributed File System), MapReduce, YARN (Yet Another Resource Negotiator), and Hive.\n",
      "2. **Spark**: Spark is a fast and general-purpose cluster computing system that can be used for a wide range of big data processing tasks, such as batch processing, interactive queries, streaming, machine learning, and graph processing. Spark provides an API for Java, Scala, Python, and R.\n",
      "3. **Data Processing and Analysis Tools**: You should learn data processing and analysis tools such as Pig, Impala, and Apache Drill. These tools provide high-level abstractions for data processing and analysis, making it easier for developers and analysts to work with large datasets.\n",
      "4. **Data Streaming**: Data streaming is an important aspect of big data processing. You should learn about data streaming technologies such as Kafka, Storm, and Spark Streaming. These technologies enable real-time processing of data streams, which is essential for many big data applications.\n",
      "5. **Cloud Platforms**: Cloud platforms such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) provide managed services for big data processing and analysis. You should learn how to use these platforms to deploy and manage big data applications.\n",
      "\n",
      "These are just a few of the many things that you can learn in the big data field. I would recommend you to choose a specific area of interest and dive deeper into it. Good luck with your learning journey!"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({'skill':'Big Data'}):\n",
    "    print(s.content,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 11, 'total_tokens': 37, 'completion_time': 0.021666667, 'prompt_time': 0.001505024, 'queue_time': 0.014133766999999998, 'total_time': 0.023171691}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-fc90c90b-72a4-4207-bd02-9e5ebced9b98-0', usage_metadata={'input_tokens': 11, 'output_tokens': 26, 'total_tokens': 37})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental framework to building chains with LangChain Expression Language (LCEL), with branching and merging chains as examples\n",
    "\n",
    "> https://medium.com/@james.li/mental-model-to-building-chains-with-langchain-expression-language-lcel-with-branching-and-36f185134eac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 — Split into 2 branches and combine\n",
    "\n",
    "\n",
    "Given a question, ask different LLM models for answer, then combine the viewpoints to form a final answer.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OiX1zwXXo8XGOjLYh57P8Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"{question}\")\n",
    "combine_answers_template = \"\"\"\n",
    "\n",
    "Given the question and the answer from 2 different LLM systems (llm_1 and llm_2), pick the answer which better addresses the user query.\n",
    "Things to judge:\n",
    "1. The core answer - how relevant it is to user query\n",
    "2. Tone and format\n",
    "3. Explanations and other insights provided\n",
    "\n",
    "Output the judgement in a valid JSON as shown below\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"winner\": \"winner llm (llm_1 or llm_2)\"\n",
    "    \"reason\": \"reason why the winner answer is better\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer llm_1:\n",
    "{answer_1}\n",
    "\n",
    "Answer_llm_2:\n",
    "{answer_2}\n",
    "\n",
    "Output:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "judge_answers_prompt = PromptTemplate(template=combine_answers_template, input_variables=['answer_1', 'answer_2', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n\\nGiven the question and the answer from 2 different LLM systems (llm_1 and llm_2), pick the answer which better addresses the user query.\\nThings to judge:\\n1. The core answer - how relevant it is to user query\\n2. Tone and format\\n3. Explanations and other insights provided\\n\\nOutput the judgement in a valid JSON as shown below\\n\\n```\\n{\\n    \"winner\": \"winner llm (llm_1 or llm_2)\"\\n    \"reason\": \"reason why the winner answer is better\"\\n}\\n```\\n\\nQuestion:\\nq\\n\\nAnswer llm_1:\\nanswer_1\\n\\nAnswer_llm_2:\\nanswer_2\\n\\nOutput:\\n\\n')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_answers_prompt.invoke({\n",
    "    \"question\": \"q\",\n",
    "    \"answer_1\": \"answer_1\",\n",
    "    \"answer_2\": \"answer_2\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = llm\n",
    "model2 = llm2\n",
    "model3 = llm_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | model1 | StrOutputParser()\n",
    "chain2 = prompt | model2 | StrOutputParser()\n",
    "chain_judge = judge_answers_prompt | model3 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test each subchain separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's the best way to stay up to date with latest Large Language Model news? Please keep the answer short and concise, limit to 3 bullet points.\"\n",
    "\n",
    "answer_1 = chain1.invoke(question)\n",
    "answer_2 = chain2.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Follow reputable AI research organizations and leading researchers in the field on social media, such as Twitter.\n",
      "2. Regularly check AI-focused news outlets and blogs for updates on large language models.\n",
      "3. Consider subscribing to relevant newsletters or email updates from organizations such as Hugging Face, OpenAI, and Google Research.\n"
     ]
    }
   ],
   "source": [
    "print (answer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three ways to stay up to date with the latest Large Language Model (LLM) news:\n",
      "\n",
      "• **Follow industry leaders and researchers on Twitter**: Many prominent researchers and industry leaders in the field of LLMs share their latest findings, updates, and insights on Twitter. Follow accounts like @huggingface, @nytimes, @MIT_CSAIL, and @StanfordNLP.\n",
      "\n",
      "• **Subscribe to AI and NLP newsletters**: Newsletters like The AI Alignment Newsletter, The NLP Newsletter, and AI in Industry provide regular updates on the latest developments in LLMs and related fields.\n",
      "\n",
      "• **Attend online conferences and webinars**: Attend online conferences and webinars focused on LLMs and NLP to stay up to date with the latest research and advancements. Some popular events include the annual Conference on Neural Information Processing Systems (NIPS) and the International Conference on Computational Linguistics (COLING).\n"
     ]
    }
   ],
   "source": [
    "print (answer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output = chain_judge.invoke({\n",
    "    \"question\": question,\n",
    "    \"answer_1\": answer_1,\n",
    "    \"answer_2\": answer_2\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'winner': 'llm_2',\n",
       " 'reason': 'The answer from llm_2 provides more specific and actionable suggestions, including the names of industry leaders and researchers to follow on Twitter, and specific newsletters and conferences to attend. The answer from llm_1 is more general and lacks the same level of detail and specificity.'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(eval_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"answer_1\": chain1,\n",
    "    \"answer_2\": chain2\n",
    "} | chain_judge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the output:\n",
      "\n",
      "{\n",
      "\"winner\": \"llm_2\",\n",
      "\"reason\": \"The answer from llm_2 provides more specific and actionable suggestions, including the names of prominent researchers and newsletters. The format is also more engaging, with bullet points and a clear structure. Additionally, llm_2's answer provides more insights and resources, such as online forums and discussion groups, which can be valuable for users looking to stay up-to-date with the latest LLM news.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "combined_answer = combined_chain.invoke(question)\n",
    "print (combined_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tips: You can use `RunnablePassthrough.assign(input_variable: subchain)` to add additional variables as you move down the chain\n",
    "\n",
    "You can think of `RunnablePassthrough.assign(foo=bar_chain)` as an additional dictionary item `{ \"foo\": \"output_from_bar_chain\" }`.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrCEcZU_FXlJzvygP4XFYw.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnablePassthrough.assign(answer_1=chain1)\n",
    "    | RunnablePassthrough.assign(answer_2=chain2)\n",
    "    | chain_judge\n",
    ")\n",
    "\n",
    "# This is equivalent to \n",
    "# {\n",
    "#    \"question\": RunnablePassthrough(),\n",
    "#    \"answer_1\": chain1,\n",
    "#    \"answer_2\": chain2,\n",
    "# } | chain_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"winner\": \"llm_2\",\n",
      "\"reason\": \"The answer from llm_2 provides more specific and actionable information, including specific Twitter handles and newsletters, which makes it easier for the user to stay up-to-date with the latest LLM news. Additionally, the answer from llm_2 provides more context and explanations, such as the importance of attending conferences and webinars, which makes it a more comprehensive and informative answer.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "combined_answer = combined_chain.invoke(question)\n",
    "print (combined_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 — Split into n branches and merge\n",
    "\n",
    "\n",
    "In this example, given a user question, the goal is for the LLM to come up with an answer which considers both pros and cons to the question.\n",
    "\n",
    "- The LLM first outputs a list of bullet points for a given question.\n",
    "- For each bullet point, we ask the LLM to generate some pros and cons in parallel.\n",
    "- Finally, we take into account the arguments on both sides to reach a final answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
