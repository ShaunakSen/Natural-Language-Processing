{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of LLamaIndex\n",
    "\n",
    "> https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/mini_projects/chatgpt_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, SimpleDirectoryReader, ServiceContext,\n",
    "    set_global_service_context, StorageContext, load_index_from_storage,\n",
    "    Document\n",
    ")\n",
    "from creds import AZURE_API_BASE, AZURE_API_KEY, AZURE_API_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    deployment_name='gpt-4-32k',\n",
    "    model='gpt-4-32k',\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_API_BASE,\n",
    "    api_version=AZURE_API_VERSION,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_API_BASE,\n",
    "    api_version=AZURE_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/gpt-4-32k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known as the co-founder of the influential startup accelerator and seed capital firm Y Combinator, which has funded and mentored numerous successful startups, including Dropbox, Airbnb, and Reddit.\n",
      "\n",
      "Graham was born in Weymouth, England, in 1964, and moved to the United States with his family when he was a child. He studied at Cornell University, where he earned a Bachelor of Arts in Philosophy and a Bachelor of Science in Computer Science. He later earned a Master of Science in Computer Science from Harvard University and a Ph.D. in Applied Sciences, also from Harvard.\n",
      "\n",
      "In the 1990s, Graham co-founded Viaweb, an early e-commerce platform that allowed users to create online stores. Viaweb was acquired by Yahoo! in 1998 and became Yahoo! Store. After the acquisition, Graham began focusing on writing essays and books on topics such as programming, startups, and technology. Some of his notable works include the books \"Hackers & Painters: Big Ideas from the Computer Age\" and \"On Lisp: Advanced Techniques for Common Lisp.\"\n",
      "\n",
      "In 2005, Graham co-founded Y Combinator with his then-wife Jessica Livingston, Robert Morris, and Trevor Blackwell. Y Combinator has since become one of the most prestigious and successful startup accelerators in the world, providing funding, mentorship, and resources to hundreds of startups.\n",
      "\n",
      "Graham is also known for his essays on his personal website, which cover a wide range of topics, including technology, startups, culture, and education. His writing has been influential in the tech and startup communities and has been widely cited and discussed.\n"
     ]
    }
   ],
   "source": [
    "# non-streaming\n",
    "resp = llm.complete(\"Paul Graham is \")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 346.87file/s]\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./data/\").load_data(show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'data/paul_graham_essay.txt',\n",
       " 'file_name': 'paul_graham_essay.txt',\n",
       " 'file_type': 'text/plain',\n",
       " 'file_size': 75042,\n",
       " 'creation_date': '2023-11-24',\n",
       " 'last_modified_date': '2023-11-24',\n",
       " 'last_accessed_date': '2023-11-24'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This builds an index over the documents in the `data`` folder \n",
    "(which in this case just consists of the essay text, but could contain many documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/gpt-4-32k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, the author mainly worked on writing and programming outside of school. They wrote short stories and experimented with programming on the IBM 1401 using an early version of Fortran. Later, they started programming on a TRS-80, creating simple games, a program to predict model rocket heights, and a word processor.\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the data you just loaded is stored in memory as a series of vector embeddings. You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you don’t get the benefits of persisting unless you load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.indices.loading:Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"storage\"):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist()\n",
    "\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Concepts\n",
    "\n",
    "> https://gpt-index.readthedocs.io/en/stable/getting_started/concepts.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation (RAG)\n",
    "\n",
    "LLMs are trained on enormous bodies of data but they aren’t trained on your data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to. You will see references to RAG frequently in this documentation.\n",
    "\n",
    "In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
    "\n",
    "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application.\n",
    "\n",
    "![](https://gpt-index.readthedocs.io/en/stable/_images/basic_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stages within RAG\n",
    "\n",
    "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
    "\n",
    "- Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
    "\n",
    "- Indexing: this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
    "\n",
    "- Storing: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "\n",
    "- Querying: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "\n",
    "- Evaluation: a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n",
    "\n",
    "![](https://gpt-index.readthedocs.io/en/stable/_images/stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stage\n",
    "\n",
    "__Documents/Nodes__:\n",
    "\n",
    "> https://gpt-index.readthedocs.io/en/stable/module_guides/loading/documents_and_nodes/root.html\n",
    "---\n",
    "\n",
    "A Document is a generic container around any data source - for instance, a PDF, an API output, or retrieved data from a database. They can be __constructed manually__, or __created automatically via our data loaders__. By default, a Document stores text along with some other attributes. Some of these are listed below.\n",
    "\n",
    "- metadata - a dictionary of annotations that can be appended to the text.\n",
    "- relationships - a dictionary containing relationships to other Documents/Nodes.\n",
    "\n",
    "\n",
    "A Node represents a “chunk” of a source Document, whether that is a text chunk, an image, or other. Similar to Documents, they contain metadata and relationship information with other nodes.\n",
    "\n",
    "\n",
    "Nodes are a first-class citizen in LlamaIndex. You can choose to define Nodes and all its attributes directly. You may also choose to “parse” source Documents into Nodes through our NodeParser classes. By default every Node derived from a Document will inherit the same metadata from that Document (e.g. a “file_name” filed in the Document is propagated to every Node).\n",
    "\n",
    "__Connectors:__\n",
    "\n",
    " A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"hi my name is mini\", \"i am 28 yrs old\", \"i ate tender cocounut ice cream today. i also like coconut water\"]\n",
    "documents = [Document(text=t) for t in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/gpt-4-32k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the mini eat?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi my name is mini\n",
      "i am 28 yrs old\n",
      "i ate tender cocounut ice cream today. i also like coconut water\n"
     ]
    }
   ],
   "source": [
    "for node_ in nodes:\n",
    "    print (node_.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/text-embedding-ada-002/embeddings?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://ml-dev-3.openai.azure.com//openai/deployments/gpt-4-32k/chat/completions?api-version=2023-03-15-preview \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(nodes)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the mini eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini ate tender coconut ice cream today.\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Stage\n",
    "\n",
    "An `Index`` is a __data structure that allows us to quickly retrieve relevant context for a user query__. For LlamaIndex, it’s the core foundation for retrieval-augmented generation (RAG) use-cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaunak_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
