{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of LLamaIndex\n",
    "\n",
    "> https://gpt-index.readthedocs.io/en/stable/getting_started/starter_example.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/mini_projects/chatgpt_clone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from llama_index.llms import AzureOpenAI\n",
    "from llama_index.embeddings import AzureOpenAIEmbedding\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, SimpleDirectoryReader, ServiceContext,\n",
    "    set_global_service_context, StorageContext, load_index_from_storage\n",
    ")\n",
    "from creds import AZURE_API_BASE, AZURE_API_KEY, AZURE_API_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    deployment_name='gpt-4-32k',\n",
    "    model='gpt-4-32k',\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_API_BASE,\n",
    "    api_version=AZURE_API_VERSION,\n",
    ")\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_API_BASE,\n",
    "    api_version=AZURE_API_VERSION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an English-born computer scientist, entrepreneur, venture capitalist, author, and essayist. He is best known as the co-founder of the influential startup accelerator and seed capital firm Y Combinator, which has helped launch and fund numerous successful technology companies, including Dropbox, Airbnb, and Reddit.\n",
      "\n",
      "Graham was born in Weymouth, England, in 1964, and moved to the United States with his family when he was a child. He earned a Bachelor of Arts in Philosophy from Cornell University, a Master of Science in Applied Mathematics from Harvard University, and a Ph.D. in Computer Science from Harvard University.\n",
      "\n",
      "Before founding Y Combinator, Graham co-founded Viaweb, an early e-commerce platform that was acquired by Yahoo! in 1998 and became Yahoo! Store. He has also written several programming books, including \"ANSI Common Lisp\" and \"On Lisp,\" and is known for his essays on technology, startups, and culture, which are published on his website, paulgraham.com.\n",
      "\n",
      "In addition to his work with Y Combinator, Graham is an active angel investor and has invested in numerous technology startups. He is also a frequent speaker at technology and entrepreneurship conferences and events.\n"
     ]
    }
   ],
   "source": [
    "# non-streaming\n",
    "resp = llm.complete(\"Paul Graham is \")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.readers.file.base:> [SimpleDirectoryReader] Total files added: 1\n",
      "> [SimpleDirectoryReader] Total files added: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1/1 [00:00<00:00, 388.94file/s]\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(input_dir=\"./data/\").load_data(show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/data/paul_graham_essay.txt',\n",
       " 'file_name': 'paul_graham_essay.txt',\n",
       " 'file_type': 'text/plain',\n",
       " 'file_size': 75042,\n",
       " 'creation_date': '2023-11-24',\n",
       " 'last_modified_date': '2023-11-24',\n",
       " 'last_accessed_date': '2023-11-24'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This builds an index over the documents in the `data`` folder \n",
    "(which in this case just consists of the essay text, but could contain many documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, the author mainly worked on writing and programming outside of school. They wrote short stories and experimented with programming on the IBM 1401 using an early version of Fortran. Later, they started programming on a TRS-80, creating simple games, a program to predict model rocket flight, and a word processor.\n"
     ]
    }
   ],
   "source": [
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the data you just loaded is stored in memory as a series of vector embeddings. You can save time (and requests to OpenAI) by saving the embeddings to disk. That can be done with this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/docstore.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/docstore.json\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/index_store.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/index_store.json\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/graph_store.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/graph_store.json\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/default__vector_store.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/default__vector_store.json\n"
     ]
    }
   ],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you don’t get the benefits of persisting unless you load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/docstore.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/docstore.json\n",
      "DEBUG:llama_index.storage.kvstore.simple_kvstore:Loading llama_index.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "Loading llama_index.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/index_store.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/index_store.json\n",
      "DEBUG:llama_index.graph_stores.simple:Loading llama_index.graph_stores.simple from ./storage/graph_store.json.\n",
      "Loading llama_index.graph_stores.simple from ./storage/graph_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/graph_store.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/graph_store.json\n",
      "DEBUG:llama_index.vector_stores.simple:Loading llama_index.vector_stores.simple from ./storage/default__vector_store.json.\n",
      "Loading llama_index.vector_stores.simple from ./storage/default__vector_store.json.\n",
      "DEBUG:fsspec.local:open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/default__vector_store.json\n",
      "open file: /Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/storage/default__vector_store.json\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"storage\"):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist()\n",
    "\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaunak_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
