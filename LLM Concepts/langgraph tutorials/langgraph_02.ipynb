{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Union\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from loguru import logger\n",
    "import operator\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"/Users/shaunaksen/Documents/personal-projects/Natural-Language-Processing/LLM Concepts/llamaindex_tutorials/knowledge_graphs/.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "        deployment_name=\"gpt-4-turbo-0125\",\n",
    "        model=\"gpt-4-turbo-0125\",\n",
    "        openai_api_type=\"azure\",\n",
    "        azure_endpoint=os.environ['AZURE_API_BASE'],\n",
    "        openai_api_key=os.environ['AZURE_API_KEY'],\n",
    "        openai_api_version=os.environ['AZURE_API_VERSION'],\n",
    "        max_retries=2,\n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-7c47ce35-9f06-43b8-ae20-57d7a7c2f98d-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import END, MessageGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = MessageGraph()\n",
    "graph.add_node(\"oracle\", llm)\n",
    "graph.add_edge(\"oracle\", END)\n",
    "\n",
    "graph.set_entry_point(\"oracle\")\n",
    "runnable = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langgraph.graph.state.CompiledStateGraph"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(runnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +--------+   \n",
      "  | oracle |   \n",
      "  +--------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n"
     ]
    }
   ],
   "source": [
    "runnable.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 1 + 1?', id='bebd2a98-46c0-4585-bb9e-13ec81727434'),\n",
       " AIMessage(content='1 + 1 equals 2.', response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 15, 'total_tokens': 23}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-ea7f3fb7-28ec-4ebd-b57d-fc5cde72c2b7-0')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke(HumanMessage(\"What is 1 + 1?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did we do here? Let's break it down step by step:\n",
    "\n",
    "1. First, we initialize our model and a MessageGraph.\n",
    "\n",
    "2. Next, we add a single node to the graph, called \"oracle\", which simply calls the model with the given input.\n",
    "\n",
    "3. We add an edge from this \"oracle\" node to the special string END. This means that execution will end after current node.\n",
    "\n",
    "4. We set \"oracle\" as the entrypoint to the graph.\n",
    "\n",
    "5. We compile the graph, ensuring that no more modifications to it can be made.\n",
    "\n",
    "Then, when we execute the graph:\n",
    "\n",
    "\n",
    "1. LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, \"oracle\".\n",
    "\n",
    "2. The \"oracle\" node executes, invoking the chat model.\n",
    "\n",
    "3. The chat model returns an AIMessage. LangGraph adds this to the state.\n",
    "\n",
    "4. Execution progresses to the special END value and outputs the final state.\n",
    "\n",
    "\n",
    "And as a result, we get a list of two chat messages as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langgraph - the easy way\n",
    "\n",
    "> Notes on tutorial by Menlo Park Lab: https://www.youtube.com/watch?v=R8KB-Zcynxc&t=173s\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1: str) -> str:\n",
    "    return input_1 + \" Hi \"\n",
    "\n",
    "def function_2(input_2: str) -> str:\n",
    "    return input_2 + \"there:)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a langchain graph\n",
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge(\"node_1\", \"node_2\")\n",
    "\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +--------+   \n",
      "  | node_1 |   \n",
      "  +--------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +--------+   \n",
      "  | node_2 |   \n",
      "  +--------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello Hi there:)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_1': 'hello Hi '}\n",
      "{'node_2': 'hello Hi there:)'}\n"
     ]
    }
   ],
   "source": [
    "input = \"hello\"\n",
    "for output in app.stream(input):\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hey there\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example with LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input_1: str) -> str:\n",
    "    response = llm.invoke(input_1)\n",
    "    return response.content\n",
    "\n",
    "def function_2(input_2: str) -> str:\n",
    "    return \"Agent says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", function_1)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge(\"agent\", \"node_2\")\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+  \n",
      "| __start__ |  \n",
      "+-----------+  \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +-------+    \n",
      "  | agent |    \n",
      "  +-------+    \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      "  +--------+   \n",
      "  | node_2 |   \n",
      "  +--------+   \n",
      "      *        \n",
      "      *        \n",
      "      *        \n",
      " +---------+   \n",
      " | __end__ |   \n",
      " +---------+   \n"
     ]
    }
   ],
   "source": [
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Agent says: Why don't skeletons fight each other?\\n\\nThey don't have the guts.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': \"Why don't skeletons fight each other?\\n\\nThey don't have the guts.\"}\n",
      "{'node_2': \"Agent says: Why don't skeletons fight each other?\\n\\nThey don't have the guts.\"}\n"
     ]
    }
   ],
   "source": [
    "input = \"tell me a joke\"\n",
    "for output in app.stream(input):\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Get weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_extractor(input: str) -> str:\n",
    "    prompt = f\"Your task is to extract the city name from the user query. Nothing more, just the name if the city.\\\n",
    "        User query:\" + input\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(input_2: str) -> str:\n",
    "    return \"Agent says: \" + input_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", city_extractor)\n",
    "workflow.add_node(\"node_2\", function_2)\n",
    "\n",
    "workflow.add_edge(\"agent\", \"node_2\")\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"node_2\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent says: Bangalore'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"Whats the weather like in Bangalore?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import OpenWeatherMapAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = OpenWeatherMapAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather.run(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Bangalore, the current weather is as follows:\n",
      "Detailed status: few clouds\n",
      "Wind speed: 2.57 m/s, direction: 220°\n",
      "Humidity: 55%\n",
      "Temperature: \n",
      "  - Current: 30.8°C\n",
      "  - High: 30.9°C\n",
      "  - Low: 30.8°C\n",
      "  - Feels like: 33.29°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 20%\n"
     ]
    }
   ],
   "source": [
    "print (weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_from_city(city_name: str) -> str:\n",
    "    \n",
    "    try:\n",
    "        weather_data = weather.run(city_name)\n",
    "    except:\n",
    "        return f\"Weather not found for city: {city_name}\"\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"agent\", city_extractor)\n",
    "workflow.add_node(\"weather_tool\", get_weather_from_city)\n",
    "\n",
    "workflow.add_edge(\"agent\", \"weather_tool\")\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.set_finish_point(\"weather_tool\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Kolkata, the current weather is as follows:\n",
      "Detailed status: haze\n",
      "Wind speed: 5.14 m/s, direction: 200°\n",
      "Humidity: 49%\n",
      "Temperature: \n",
      "  - Current: 32.97°C\n",
      "  - High: 32.97°C\n",
      "  - Low: 32.97°C\n",
      "  - Feels like: 35.95°C\n",
      "Rain: {}\n",
      "Heat index: None\n",
      "Cloud cover: 40%\n"
     ]
    }
   ],
   "source": [
    "print(app.invoke(\"Whats the weather like in Kolkata?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shaunak_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
